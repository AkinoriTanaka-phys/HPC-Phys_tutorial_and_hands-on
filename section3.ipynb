{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('bmh')\n",
    "%matplotlib inline\n",
    "\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container {width:100% !important;}</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# section1, 2 で実装したもので必要なもの\n",
    "from maze import MazeEnv\n",
    "from agents import Agent\n",
    "from policies import Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 方策最適化に基づく学習アルゴリズム\n",
    "価値ベースの方法は、「行動空間」`Env.action_space` が巨大すぎる場合などではトレーニングに失敗することがあります。別の選択肢の 1 つは、値 Q(s,a) ではなく、ポリシー $\\pi$ 自体のモデルを作成することです。\n",
    "\n",
    "## 3-1. 状態価値関数、期待収益、方策勾配法\n",
    "[Section2](https://github.com/AkinoriTanaka-phys/HPC-Phys_tutorial_and_hands-on/blob/master/section2.ipynb)\n",
    "では行動価値関数を\n",
    "\n",
    "$$\n",
    "Q^\\pi(s,a) = \\langle \\sum_{k=0}^\\infty \\gamma^k r_{t+1+k} \\rangle_{(s_t, a_t)=(s,a)}\n",
    "$$\n",
    "\n",
    "としました。これは状態 s で行動 a を取った時の収益の期待値だったわけですが、これを更に a について方策で期待値を取ったもの\n",
    "\n",
    "$$\n",
    "V^{\\pi}(s) = \\langle Q^{\\pi}(s, a) \\rangle_{a \\sim \\pi(\\cdot|s)}\n",
    "$$\n",
    "\n",
    "これは「方策 $\\pi$ のもとで状態 s が持っている価値」を表し、**状態価値関数(state value function)** と呼ばれます。更に、迷路の実装を思い出すと"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Env = MazeEnv(5,5, figsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS4AAADVCAYAAADpYh0tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGShJREFUeJzt3X10VPW97/H3dxKEACE9oKlIHCBFatUckQdPseFKSKvW\n49IuFY89slSWFZUqPiwsVSvndlmsuO46anGVatNWfMAHcjyl96C3HgJRRpQrRKwXcXkw1BTlSRAN\nBM3T7/6RhxM0QDIPv8ne+/Nai7WyZ2bv72/vYb7z29/Z8x1zziEiEiSxbA9ARKS3lLhEJHCUuEQk\ncJS4RCRwlLhEJHBysz2AL6uqqsrIx5wbN25k3Lhxmdh0n42tfU6/8vJyy9jGpcf6XOICGD9+fNq3\n+dJLL2Vku305tvY5vWpqajKyXek9nSqKSOBEJnHV1dVFLrb2WcIqMomrpKQkcrG1zxJW1te+8lNV\nVeWyVZcROZKamhoV5/uIPlmcFwmb1atXjxw4cOC/5ubm/h0ROtNJhnOupampadXkyZMXHO4xkUlc\niUSC0tLSSMXWPvcNq1evHjlkyJAXRo0adUosppzVE7t37y5Zs2bNuilTpqzs7v5AJa78/HxycnKS\nWnfw4MF87WtfS/OIMh87lVP5QYMGUVBQkNS6ubmp/ddwzmGW3FlVS0tL0nFTOdb79u1LOu6RDBw4\n8F+VtHrn2GOPPW7fvn23AsFPXEDSL4aysrI0j6Tvx04lbjpqn8luI9nnGLL7PB9Obm7u3ylp9Y6Z\nkZOTk3e4+3U0RTIvtddZczP2ySdpGkqgHPa4RSZxVVdXRy52Nvc5W8K4z/0feYS8226DPnYFQDZF\nJnGJBJHt2kW/FSuI1dbS79//PdvD6bGFCxdy/fXXZ2z7kUlcU6dOjVzsbO5ztoRtnwfcey/W0IDl\n5HDMk0/CgQPZHlKPpVKrPJrIJC6RoMlJJMh96y1oL+zHPv6YAQ88kOVR9Q2RSVyqcUVDaPa5qYkB\nDz986G25ufR75RViW7akNdRbb73F1KlTGTlyJDNnzuSaa67h3nvvBWDJkiVMnDiRMWPGMGPGDHbs\n2NG53h133EFJSQkjR46kvLyc119/Pa3jOpLIJC6RIOn/6KPEPvoIvny61dTEgPvuS1uhvqmpiSuv\nvJIrrriC2tpaLrnkElasWAHAmjVr+MUvfsFjjz3G5s2bKSoq4kc/+lHnuhMmTCCRSLB161YuueQS\nZs6cSWNjY1rGdTSRSVyqcUVDGPa5oyBPdxcBmxHbsoV+f/xjWmKtX7+elpYWrr32WnJycrjgggs6\n+5ktW7aMGTNmcNppp9GvXz/uvvtu3njjDbZt2wbApZdeSkFBAbFYjNmzZ/PFF1+wJc2zwcMJ3AWo\nvdHa2kpVVRWVlZUcPHiQvLw8pk+fzrRp09AFgdJnffEFdoRvD5hzWH19WkJt376d4cOHH3LbiBEj\nANixY8ch3WQHDRrE0KFD+eijjygqKmLRokU89dRT7Ny5E4D9+/ezZ8+etIzraLy+es3sPDN718ze\nM7N5mYy1d+9eLrvsMm6//XbWr1/PunXrWL9+PXPnzuWyyy5j7969mQx/CNW4/AnDPrsTT6SprAya\nm7u509Eaj9M4Y0ZaYh1//PFs3779kNs+/PBDAIYPH35If7MDBw6wd+9eTjjhBF5//XUefvhhHnvs\nMbZu3crWrVvJz89PyzcuesJb4jKzGPAwcC5wKvBDMzs5E7FaW1uZNWsWtbW15Obmdn4sa2bk5uZS\nW1vLrFmzaG1tzUR4kZR9fvPNtBYWfuV2F4txcO7czk8aUzVp0iRycnKoqKigpaWFF154obNF9cUX\nX8zTTz/Npk2b+OKLL7jnnnuYNGkSRUVF1NfXk5uby9ChQ2lsbOT+++9n//79aRlTT/iccZ0J/Jdz\n7gPnXBPwDHBRJgKtWrWKLVu2HHI6mJ+f3/l3LBZjy5YtrF69OhPhv0I1Ln9Cs895eTTOnAld31xb\nWmiZNInW009PW5h+/frx+OOP88QTTzB69GgqKys599xz6d+/P2effTZ33HEHV155Jaeeeip1dXX8\n9re/BaC8vJxp06YxadIkzjjjDPLy8jpPMX3wWeMaAfyty/I22pJZ2i1btuyoXSRycnJYtmwZ5eXl\nmRiCSMqazj+ffn/6EznvvQdmuIICPv/pT9Me5/TTT+fll1/uXP7e977HeeedB8DVV1/N1Vdf/ZV1\nYrEYixYtYtGiRZ233XTTTZ1/z5uX0UpQ3yvOV1ZWUlFRQTweB6CgoICSkpLOHksdNYyOd9bulrdu\n3dp5eljfpYiZn5/fuZyfn09DQ0OPtpfq8saNG7nlllsytv3DLXet9/R2/Wyqrq5Oev8ffPBBxo0b\nl/T6iUQCgNLSUhKJBEuXLgUgHo9TWFjo943OjIN33cXga6+FgwdpvPhiXAZaM61du5YxY8YwbNgw\nnnvuOTZv3tzn39C9tW42s28D/9M5d1778k8B55xb2PVxR2rdnJ+f36M+Uddddx3r168/5CsH9fX1\nh5wuOueYNGkSv/nNb5LZnV7p+kLsrVSen1TiZvNT12zt8ydH6cCQbOvmDRs2VBcXF5+d1KCAAffd\nR85f/sKBJ59MW22rqyVLlvDLX/6ShoYGRo0axfz58/nud7+b9ji9VVtb+/KECROmdnefzxnXG8AY\nMxsJbAcuB36YiUDTp09n3bp1hyS5rkkL2prVTZ8+PRPhv0I1Ln/CuM+f33IL9vHHGUlaAFdddRVX\nXXVVRradKd7eVp1zLcCNwEvAJuAZ59zmTMSaNm0aY8aMOeynhq2trYwZM6ZPNp0T+YoBA3BFRdke\nRZ/i9XzAOfd/nHPfdM6d5Jy7L1NxYrEYjz76KMXFxTQ3N+Oco76+Hucczc3NFBcX8+ijj3o7HdJ1\nXP5EcZ+jqM8V59Nl6NChPPfcc6xevZply5ZRW1tLcXEx06dPp6ysTFfOiwRYaBMXtM28ysvLs/4J\niWpc/kRxn6NI0w4RCZzIJC7144qGsO1za2srdXV11NbW0tDQkO3h9BmhPlUUCaqWlhYWL17MK6+8\nwq5du3DOMWTIEE477TTmzp3LsGHDsj3ElFx44YVcdtllzEjyy+KRmXGpH1c0hGGfW1pauO2223j2\n2Wf5+OOPicVi5OTkcODAAV577TWuvfZadu3ale1hZlVkEpdIUDzxxBPU1NTQr1+/r9wXi8XYu3cv\nP//5z9MW76GHHmLChAnE43HOOuuszg6ora2t/OxnP+Okk05i/PjxVFRUMGzYsM7rIz/77DPmzJnD\nKaecwmmnncaCBQs6v/Xw9NNPc/755zN//nyKi4sZP348VVVVACxYsIDXXnuNefPmEY/H+WkS37+M\nTOJSjSsagr7PzjlWrlzZbdLqEIvFeO+9977SRytZo0eP5sUXX6Suro6f/OQn3HDDDezatYslS5aw\natUq1qxZQ3V1NStWrDjka3Q//vGPOeaYY6ipqeHll1+murqaxx9/vPP+mpoaxo4dy/vvv89NN93E\nnDlzALjrrruYPHkyCxcupK6ujvvu6/0lnZFJXCJBUF9fz+7du4/6uIMHD/Lqq6+mJeaFF15IYXvv\nrx/84AeMHj2aDRs2sHz5cq677jqOP/54hgwZ0tkoAGDXrl2sXLmSBQsWMGDAAIYNG8b111/P888/\n3/mYE088kRkzZmBmXH755ezYsaNH+9YTkSnOq8YVDUHfZ+dcj79knq4GCc888wyLFy/u7Hba0NDA\nnj172L59+yE9trr+vW3bNpqamvjWt751yLiLunw1qbBLI8S8vDygrYvqcccdl/KYA5W4xo0bx6ZN\nm7ISO5M/bnkk2erSqu6w2ZGfn8/QoUOPWnzv378/EydOTDnetm3buPXWW1m+fDlnntnWHu/ss9sa\nWQwfPpyPPvrokMd2GDFiBAMGDOD9999P6rWR6utJp4oe+God9GWq6wVPLBajtLSU5u76zbdzzlFc\nXMzo0aNTjnfgwAFisVhn0f2pp55i8+a23gcXXXQRjzzyCNu3b+fTTz/lV7/6Ved6X//61ykrK+PO\nO+/s/B7wX//6V9auXdujuMcddxwffPBB0uNW4hLpY2bNmsXYsWO7TV7OOQYNGpTUJ3Hd+eY3v8ns\n2bM555xzOPnkk3n33Xf59re/DbS1uykrK2PKlCmUlZVxzjnnkJub2/k931//+tc0NTUxefJkiouL\nmTlzZucv/nSn6yzruuuuY/ny5XzjG9/gjjvu6PW4vTUS7KkjNRKcMmWKThU9yeb/i2wd677USPDz\nzz/n/vvvZ8OGDZ2njYMHD2bs2LHcfvvtjBo1qrfDSNnKlSuZO3cuGzdu9BKvrzQSFJEeGjBgAPPn\nz6ehoYF33nmHxsZGTjrppLQUtnvq888/Z82aNUybNo2dO3dy//33c8EFF3iLfyQ6VfRANa7wx82U\ngQMHMnHiRM466yyvSQva/t8uXLiQ4uJipk2bxsknn5y2U9RUacYlIt3Ky8tj5cqV2R5GtzTj8iBb\nNRtduyZhpcQlkmHtv7cgvXfYT6aUuDxQjSv8cY+kqalp1e7du3f3tU/w+7KGhobGxsbGtw93v2pc\nIhk2efLkBWvWrFm3b9++W3NycvLQhOFoWhsbG99uaGi49XAP8Ja4zOx3wAXATufc3/uK2xeoxhX+\nuEczZcqUlUDfrHQHkM/M/wfgXI/xRCSkfP4gbAI48qXJIaUaV/jjil861xaRwFHi8kA1rvDHFb/6\n3KeKlZWVVFRUEI/HASgoKKCkpITS0tIsj6xNx2lfRzLK9HLHqU/HC9LXckdPpmzENzPv+9uxnEgk\nACgtLSWRSLB06VIA4vE4hYWFWf9xYWnjtTuEmY0C/rdzruRwjwljdwjnXNLrp9Idorq6OukZSKr/\nL1KJncqxTiVuprpDSPp5O1U0s6XAWmCsmdWZ2UxfsUUkXLydKjrn/tlXrL5GNa7wxxW/VJwXkcBR\n4vJA13GFP674pcQlIoGjxOWBalzhjyt+KXGJSOAocXmgGlf444pfSlwiEjhKXB6oxhX+uOKXEpeI\nBI4SlweqcYU/rvilxCUigdPn2tr0VanOmpJdP5W4Z599dtLrx2LRe0/bu3dvtocgPRS9/50iEnhK\nXCGmeo+ElRKXiASOEleI6ZomCSslLhEJHCWuEFONS8JKiUtEAkeJK8RU45KwUuISkcBR4gox1bgk\nrHz+rmKRma0ys01m9raZzfEVW0TCxed3FZuB25xzG81sMLDBzF5yzr3rcQyRohqXhFWPZ1xm9oCZ\njUs2kHNuh3NuY/vf+4HNwIhktyci0dWbU8Uc4M9m9v/MbJ6ZFSUb1MxGAeOAdcluQ45ONS4Jqx6f\nKjrn5pjZrcD3gSuAn5nZOuBx4Pn2WdRRtZ8mVgI3d7dOZWUlFRUVxONxAAoKCigpKaG0tLSnQw2V\njuTTcdrnazmqEokEAKWlpSQSCZYuXQpAPB6nsLCQ8vLybA5P2lmy/ZrM7FRgKVACNADPAP/inPvw\nCOvkAv8BvOice6i7x1RVVbnx48d3u/6UKVPYtGlTUuMNqtbW1qzEVT+ur6qpqaG8vDw7PyAgh+jV\n/04zG2Jm15jZauAV2k71pgDfAvYDLx5lE78H3jlc0hIR6YneFOcrgQ+Bi4HfACc452Y55151zv0N\nuA0YfYT1v0PbKeY0M3vTzGrM7LzUhi9HEvXTPgmv3lwO8Tpwo3NuR3d3Oudazezrh1vZOfcqbQV+\nEZGU9KY4/7968JiG1IYj6aTruCSsoleBFZHAU+IKMdW4JKyUuEQkcJS4Qkw1LgkrJS4RCRwlrhBT\njUvCSolLRAJHiSvEVOOSsFLiEpHA8dkBNdBaWlqSXre6ujrp2U8UuzSk0hEjlWO9b9++pOOKX9F7\nVYhI4ClxeaBakz861tGgxCUigaPE5YGup/JHxzoalLhEJHCUuDxQ3cUfHetoUOISkcBR4vJAdRd/\ndKyjQYlLRAJHicsD1V380bGOBm9f+TGz/rT9FuMx7XErnXM/9xVfRMLD24zLOfcFUOacOwMYB3zf\nzM70FT+bVHfxR8c6GryeKnb5+bL+tM26nM/4IhIOXhOXmcXM7E1gB/Cfzrk3fMbPFtVd/NGxjgbf\nM67W9lPFIuAfzOwUn/FFJByy0o/LOfeZma0GzgPe6XpfZWUlFRUVxONxAAoKCigpKaG0tDQLI/2q\njhpKxzt7T5Y3btzILbfcktT6UdS1p1Zvj9eDDz7IuHHjkl4/kUgAUFpaSiKRYOnSpQDE43EKCwsp\nLy9Pxy5Kisw5P2UmMzsWaHLOfWpmecCfgfuccy90fVxVVZUbP358t9uYMmUKmzZtyvxgu5GtRoI5\nOTlJxw2qvtpIsKamhvLycktq45JWPmdcw4ElZhaj7RT12S8nrbCK8uzJNx3raPCWuJxzbwPdT6VE\nRHpBV857oGuL/NGxjgYlLhEJHCUuD1R38UfHOhqUuEQkcJS4PFDdxR8d62hQ4hKRwFHi8kB1F390\nrKNBiUtEAkeJywPVXfzRsY4GJS4RCRwlLg9Ud/FHxzoastLWJllvvvlm1rolRK1LQyodGlJlpgYM\ncmSRmXGp9uFXto63nudoiEziEpHwiEziUu3Dr2wdbz3P0RCZxCUi4RGZxKXah1+qcUkmRSZxiUh4\nRCZxqfbhl2pckkmRSVziUXMzzJ0LKfwyksiRRCZxqfbh0eLFVD//PCxe7D20nudoiEziEk8+/hj+\n7d/gmGOgshL27Mn2iCSEvCcuM4uZWY2Z/clnXNU+PJk/H+rrmZqfD599Bnff7TW8nudoyMaM62bg\nnSzElUxbuxbWr4dY+3+rnJy25ddey+64JHS8Ji4zKwLOByp8xgXVPjKuuRkWLoT2L2dX19e33d7a\n2na7p0K9nudo8D3jegC4HXCe40qmLV4MdXXw5c4OZvDBB1kp1Et4eWtrY2b/COx0zm00s6lAt71L\nKisrqaioIB6PA1BQUEBJSQmlpaXAf7+jdtQyfC1HVY+P1wsvQG5u50xran5+2/0dyytWwI039mh7\nZpb089VxW7LrJxIJAEpLS0kkEixduhSAeDxOYWEh5eXl3Rwl8c2c8zP5MbN7gRlAM5AH5APPO+eu\n7Pq4qqoqN378+G63MXjwYPXj8qTX/bgWLYLf/x5yu3kvbG6Ga66BG2/s0aay1Y/rk08+OeL9NTU1\nlJeXq1lYH+DtVNE5d6dzLu6cKwYuB1Z9OWllUtRnThk3ezbE49D+RthZ43IORo6EG27wMgw9z9Gg\n67gkPXJyYN68//5EsUMs1nZ7xGaskllZSVzOuZedcxf6jKnrezw46yyYOBFaWtpqXC0tbcuTJ3sb\ngp7naNCMS9LrnntgyJC2v4cMaVsWSbPIJC7VPjwZNgwuuYTqPXvg0kvblj3S8xwNkUlc4tHs2TBp\nkreCvERPoH6eLBWqfXiUk8PUZ5/NSmg9z9GgGZeIBE5kEpdqH36p57xkUmQSl4iER2QSl2offqnn\nvGRSZBKXiIRHZBKXah9+qcYlmRSoyyH279+f9LoHDhzg008/TXr9vXv3Jr1uIpHobMvjUypx9+3b\nl1Ls/fv3p7yNZNTX1x+1y4MEn7e2Nj11pLY2ItmktjZ9R2ROFUUkPCKTuDo6W0YptvZZwioyievt\nt9+OXGzts4RVZBJXKoX5oMbWPktYRSZxiUh4RCZx1dXVRS629lnCqk9ex1VTU5P2bU6cODEj2+3L\nsbXPElZ97jouEZGjicypooiEhxKXiASOEpeIBE4kEpeZnWdm75rZe2Y2z2Pc35nZTjP7i6+Y7XGL\nzGyVmW0ys7fNbI6nuP3NbJ2Zvdke9198xO0SP2ZmNWb2J59xxb/QF+fNLAa8B5QDHwFvAJc75971\nELsU2A887pz7+0zH6xL3eOB459xGMxsMbAAu8rTPA51zDWaWA7wKzHHO/d9Mx22PfSswARji+weH\nxa8ozLjOBP7LOfeBc64JeAa4yEdg51wC8N5jxTm3wzm3sf3v/cBmYISn2A3tf/an7XIbL++MZlYE\nnA9U+Ign2RWFxDUC+FuX5W14ehH3BWY2ChgHrPMUL2ZmbwI7gP90zr3hIy7wAHA7nhKlZFcUEldk\ntZ8mVgI3t8+8Ms451+qcOwMoAv7BzE7JdEwz+0dgZ/ss09r/SYhFIXF9CMS7LBe13xZqZpZLW9J6\nwjm33Hd859xnwGrgPA/hvgNcaGa1wNNAmZk97iGuZEkUEtcbwBgzG2lmxwCXAz4/dcrWDOD3wDvO\nuYd8BTSzY82soP3vPOB7QMY/EHDO3emcizvniml7flc5567MdFzJntAnLudcC3Aj8BKwCXjGObfZ\nR2wzWwqsBcaaWZ2ZzfQU9zvAFcC09ksTaszMx8xnOLDazDbSVlP7s3PuBQ9xJWJCfzmEiIRP6Gdc\nIhI+SlwiEjhKXCISOEpcIhI4SlwiEjhKXCISOEpcIhI4SlwiEjhKXCISOEpcIWNmxWa2x8zGtS+f\nYGa7zOx/ZHtsIumixBUyzrla4CfAk+1fdP4D8Afn3CvZHZlI+ui7iiFlZn8EioFWYFJ791eRUNCM\nK7wqgFOBRUpaEjaacYWQmQ0C3gJWAd8HSpxz+7I7KpH0UeIKITP7HZDnnPtnM3sE+Jpz7p+yPS6R\ndNGpYsiY2YXAOcDs9ptuA84wsx9mb1Qi6aUZl4gEjmZcIhI4SlwiEjhKXCISOEpcIhI4SlwiEjhK\nXCISOEpcIhI4SlwiEjj/Hwa2AobH/trKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ebbfcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Env.reset()\n",
    "Env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "の**`Env.reset()`**は最初の状態 s をランダムに選んでいるのがわかります。この確率を $P_\\text{reset}(s)$ とし、状態価値の期待値を取ると\n",
    "\n",
    "$$\n",
    "J(\\pi) = \\langle V^{\\pi}(s) \\rangle_{s \\sim P_\\text{reset} (\\cdot)}\n",
    "$$\n",
    "\n",
    "となりますが、環境が固定されている以上この量は「方策にしか依存しない量」であり、かつ方策の良さを表す量（大きいほど良い）であると言えるでしょう。これを**期待収益(expected return)**といいます。\n",
    "\n",
    "### ● 方策勾配法\n",
    "そこで、価値の推定をすっ飛ばして、この$J(\\pi)$を直接最大化する学習が考えられます。そのうちでも最も単純なのが\n",
    "方策を θ でパラメーター化し\n",
    "\n",
    "$$\n",
    "\\pi(a|s) = \\pi_\\theta(a|s)\n",
    "$$\n",
    "\n",
    "この時の期待収益を $\\theta$ の関数と考え、勾配更新\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\eta \\nabla_\\theta J(\\pi_{\\theta})\n",
    "$$\n",
    "\n",
    "によって学習を進めてゆくことです。これを**方策勾配法(Policy-gradient method)**といいます。\n",
    "\n",
    "問題は方策勾配 $\\nabla_\\theta J(\\pi_{\\theta})$ をどうやって求めるのか、というところです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. 方策勾配定理とREINFORCEアルゴリズム\n",
    "ここで少し定義を変更させてください。今までは行動価値観数を\n",
    "\n",
    "$$\n",
    "Q^\\pi(s,a) = \\langle \\sum_{k=0}^\\infty \\gamma^k r_{t+1+k} \\rangle_{(s_t, a_t)=(s,a)}\n",
    "$$\n",
    "\n",
    "と減衰和にしていましたが、ここからは\n",
    "\n",
    "$$\n",
    "Q^\\pi(s,a) = \n",
    "\\lim_{T \\to \\infty}\n",
    "\\langle \\frac{1}{T}\\sum_{k=0}^{T-1}  r_{t+1+k} \\rangle_{(s_t, a_t)=(s,a)}\n",
    "$$\n",
    "\n",
    "と平均和の時間無限大極限に取ることにします。この場合のベルマン方程式は、\n",
    "\n",
    "$$\n",
    "Q^\\pi(s,a) = \\langle\n",
    "Q^\\pi(s_{t+1}, a_{t+1})\n",
    "\\rangle_{s_{t+1} \\sim P_s(\\cdot|s, a),\\ a_{t+1} \\sim \\pi(\\cdot | s_{t+1})}\n",
    "$$\n",
    "\n",
    "となります。これを用いて方策勾配を求めてみましょう。まず状態価値観数の方策勾配から求めると\n",
    "\n",
    "$$\n",
    "\\left. \\begin{array}{rl}\n",
    "\\nabla_\\theta \\underbrace{ V^{\\pi_\\theta}(s) }_{\\langle Q^{\\pi_\\theta} (s, a) \\rangle_{a \\sim \\pi_\\theta(\\cdot|s)}}\n",
    "&=\n",
    "\\nabla_\\theta \n",
    "\\sum_{a}\n",
    "Q^{\\pi_\\theta} (s, a)\n",
    "\\pi_\\theta(a|s)\n",
    "\\\\\n",
    "&=\n",
    "\\sum_{a}\n",
    "\\Big(\n",
    "\\underbrace{\n",
    "\\nabla_\\theta \n",
    "Q^{\\pi_\\theta} (s, a)\n",
    "}_{(*)}\n",
    "\\cdot\n",
    "\\pi_\\theta(a|s)\n",
    "+\n",
    "Q^{\\pi_\\theta} (s, a)\n",
    "\\cdot\n",
    "\\underbrace{\n",
    "\\nabla_\\theta \n",
    "\\pi_\\theta(a| s)\n",
    "}_{\\big(\\nabla_\\theta \\log \\pi_\\theta(a | s)\\big) \\cdot \\pi_\\theta(a | s)}\n",
    "\\Big)\n",
    "\\\\\n",
    "&=\n",
    "\\sum_{a}\n",
    "\\Big(\n",
    "(*)\n",
    "+\n",
    "Q^{\\pi_\\theta} (s, a)\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a | s)\n",
    "\\Big) \\cdot\n",
    "\\pi_\\theta(a|s)\n",
    "\\\\\n",
    "&=\n",
    "\\big\\langle\n",
    "(*)\n",
    "+\n",
    "Q^{\\pi_\\theta} (s, a)\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a | s)\n",
    "\\big\\rangle_{a \\sim \\pi_\\theta(\\cdot|s)}\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "となって、行動価値関数の方策勾配を計算する必要に迫られます。しかしこれはベルマン方程式のため、再帰的に計算でき"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left. \\begin{array}{rl}\n",
    "(*) &= \n",
    "\\nabla_\\theta \n",
    "\\underbrace{Q^{\\pi_\\theta} (s, a)}_{Bellman}\n",
    "\\\\\n",
    "&=\n",
    "\\nabla_\\theta \n",
    "\\langle\n",
    "Q^{\\pi_\\theta}(s_{t+1}, a_{t+1})\n",
    "\\rangle_{s_{t+1} \\sim P_s(\\cdot|s, a),\\ a_{t+1} \\sim \\pi_\\theta(\\cdot | s_{t+1})}\n",
    "\\\\\n",
    "&=\n",
    "\\nabla_\\theta \n",
    "\\sum_{s_{t+1}, a_{t+1}}\n",
    "Q^{\\pi_\\theta}(s_{t+1}, a_{t+1})\n",
    "\\pi_\\theta(a_{t+1} | s_{t+1})\n",
    "P_s(s_{t+1}|s, a)\n",
    "\\\\\n",
    "&=\n",
    "\\sum_{s_{t+1}, a_{t+1}}\n",
    "\\Big(\n",
    "\\underbrace{\n",
    "\\nabla_\\theta \n",
    "Q^{\\pi_\\theta}(s_{t+1}, a_{t+1})\n",
    "}_{(*)}\n",
    "\\cdot\n",
    "\\pi_\\theta(a_{t+1} | s_{t+1})\n",
    "+\n",
    "Q^{\\pi_\\theta}(s_{t+1}, a_{t+1})\n",
    "\\cdot\n",
    "\\underbrace{\n",
    "\\nabla_\\theta \n",
    "\\pi_\\theta(a_{t+1} | s_{t+1})\n",
    "}_{\\big(\\nabla_\\theta \\log \\pi_\\theta(a_{t+1} | s_{t+1})\\big) \\cdot \\pi_\\theta(a_{t+1} | s_{t+1})}\n",
    "\\Big)\n",
    "P_s(s_{t+1}|s, a)\n",
    "\\\\\n",
    "&=\n",
    "\\sum_{s_{t+1}, a_{t+1}}\n",
    "\\Big(\n",
    "(*)\n",
    "+\n",
    "Q^{\\pi_\\theta}(s_{t+1}, a_{t+1})\\nabla_\\theta \\log \\pi_\\theta(a_{t+1} | s_{t+1})\n",
    "\\Big) \\cdot\n",
    "\\pi_\\theta(a_{t+1} | s_{t+1})\n",
    "P_s(s_{t+1}|s, a)\n",
    "\\\\\n",
    "&=\n",
    "\\big\\langle\n",
    "(*)\n",
    "+\n",
    "Q^{\\pi_\\theta}(s_{t+1}, a_{t+1})\\nabla_\\theta \\log \\pi_\\theta(a_{t+1} | s_{t+1})\n",
    "\\big\\rangle_{a_{t+1} \\sim \\pi_\\theta(\\cdot | s_{t+1}),\\ s_{t+1} \\sim P_s(\\cdot|s, a)}\n",
    "\\\\\n",
    "&=\n",
    "\\Big\\langle\n",
    "\\sum_{k=0}^\\infty\n",
    "Q^{\\pi_\\theta}(s_{t+1+k}, a_{t+1+k})\\nabla_\\theta \\log \\pi_\\theta(a_{t+1+k} | s_{t+1+k})\n",
    "\\Big\\rangle_{MDP,\\ (s_t, a_t)=(s, a)}\n",
    "\\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "と書け、(*)をもとの V の勾配に代入すると $a_t = a$ として\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta V(s)\n",
    "=\n",
    "\\Big\\langle\n",
    "\\sum_{k=0}^\\infty\n",
    "Q^{\\pi_\\theta}(s_{t+k}, a_{t+k})\\nabla_\\theta \\log \\pi_\\theta(a_{t+k} | s_{t+k})\n",
    "\\Big\\rangle_{MDP,\\ s_t=s}\n",
    "$$\n",
    "\n",
    "更に**期待収益** $J(\\pi_\\theta)$は、これの初期状態による期待値なので、結局\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\pi_\\theta)\n",
    "=\n",
    "\\Big\\langle\n",
    "\\sum_{k=0}^\\infty\n",
    "Q^{\\pi_\\theta}(s_{t+k}, a_{t+k})\\nabla_\\theta \\log \\pi_\\theta(a_{t+k} | s_{t+k})\n",
    "\\Big\\rangle_{MDP}\n",
    "$$\n",
    "\n",
    "が得られます。これを**方策勾配定理(Policy-gradient theorem)**と言います。\n",
    "\n",
    "### ● REINFORCEアルゴリズム\n",
    "実際には、期待値はサンプル$(r_t, s_t, a_t)$によって近似され\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\pi_\\theta)\n",
    "\\approx\n",
    "\\sum_{k=0}^\\infty\n",
    "Q^{\\pi_\\theta}(s_{t+k}, a_{t+k})\\nabla_\\theta \\log \\pi_\\theta(a_{t+k} | s_{t+k})\n",
    "$$\n",
    "\n",
    "とされますが、ここで結局 $Q^{\\pi_\\theta}(s_{t+k}, a_{t+k})$ が必要になってしまいました。この行動価値関数の意味は、収益の期待値だったわけですので、これを更に報酬のサンプルで適当な T (典型的にはサンプルのエピソード長さが使われる)\n",
    "\n",
    "$$\n",
    "Q^{\\pi_\\theta}(s_{t+k}, a_{t+k})\n",
    "\\approx\n",
    "\\frac{1}{T} \\sum_{K=0}^{T-1} r_{t+k+1+K}\n",
    "$$\n",
    "\n",
    "で近似することが考えられます。これを**REINFORCEアルゴリズム**と言います。すなわち、REINFORCEアルゴリズムとは\n",
    "1. MDPサンプル$\\{(r_t, s_t, a_t)\\}_{t=0, 2, \\dots, T-1}$を得る\n",
    "2. $\n",
    "\\theta \\leftarrow \\theta + \\eta g_\\theta,\\quad \\text{where} \\\n",
    "g_\\theta = \\sum_{k=0}^\\infty\n",
    "\\Big(\n",
    "\\frac{1}{T} \\sum_{K=0}^{T-1} r_{t+k+1+K}\n",
    "\\Big)\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a_{t+k} | s_{t+k})\n",
    "$\n",
    "\n",
    "を繰り返して、良い方策$\\pi_\\theta$を探索する手法のことです。\n",
    "> **【補足】** MDPサンプルは一回の更新につきなるべく多く取ったほうが方策勾配$g_t$の精度が上がります。その場合はエピソード数$m = 0, 1, \\dots, M-1$として$g_\\theta = \n",
    "\\frac{1}{M} \\sum_{m=0}^{M-1}\n",
    "\\sum_{k=0}^\\infty\n",
    "\\Big(\n",
    "\\frac{1}{T} \\sum_{K=0}^{T-1} r_{t+k+1+K}^{(m)}\n",
    "\\Big)\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a_{t+k}^{(m)} | s_{t+k}^{(m)})$です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3. Softmax方策による実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
