{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('bmh')\n",
    "%matplotlib inline\n",
    "\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container {width:100% !important;}</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. マルコフ決定過程\n",
    "---\n",
    "### 1-1. 環境\n",
    "強化学習は教師あり学習とは異なり、データを用いません。\n",
    "代わりに**環境(Environment)**が与えられると考えます。\n",
    "ここでは事前に\n",
    "* https://github.com/AkinoriTanaka-phys/HPC-Phys_tutorial_and_hands-on/blob/master/maze.py\n",
    "\n",
    "に用意した迷路の環境を使って、実装に沿った説明を試みます。\n",
    "まずは迷路の環境を読み込んでみましょう："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"書いてください\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Env.render()`**は環境を表示させる関数です。環境の関数名はなるべく<a href=\"https://gym.openai.com\">OpenAI Gym</a>を参考にしました。\n",
    "> **【補足】** <a href=\"https://gym.openai.com\">OpenAI Gym</a> はAtari社のブロック崩しゲームを始めとした、数々のゲームやその他の強化学習環境をpythonから呼び出すためのライブラリで、無料です。pipコマンドでダウンロードできます。\n",
    "\n",
    "ここで\n",
    "* ◾は通れない壁\n",
    "* ◆ は迷路のゴール地点\n",
    "\n",
    "を表すとします。早速この迷路のスタート地点に「プレイヤー」を置いてみましょう：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"書いてください\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ● が追加されました。これがプレイヤーの位置を表します。その座標(**状態(state)**といいます)は以下で確認できます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"書いてください\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "プレイヤーは [↑、↓、←、→] を各座標で選択します。これを**行動(action)**と言います。行動のリストは："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"書いてください\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後の便宜のため、[↑、↓、←、→] は `[0, 1, 2, 3]` で表現しています："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"書いてください\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "試しに ● を動かしてみましょう。それぞれ\n",
    "* `Env.step0(s, a)`：**状態:`s`**に居るときに**行動:`a`**を取ったときの次の**状態**を返す\n",
    "* `Env.step1(s, a, next_s)`：**状態:`s`**に居るときに**行動:`a`**を取り、**状態:`next_s`**に移った時の「**報酬(reward)**」の値を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"書いてください\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "真ん中の3行を毎回書くのは面倒なので\n",
    "* `Env.step(a)`：上２つを同時に実行し、(**状態:`next_s`**, **報酬:`next_r`**, 解けたかどうか, 補足)を返す\n",
    "\n",
    "を用意しました："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"書いてください\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ● ここまでのまとめ\n",
    "- 登場する集合とその要素\n",
    " - **時刻**(状態、報酬、行動の添字) <br> $\\quad T=\\{0,1,2,3, \\dots\\}=\\{t\\}$<br/>\n",
    " - **状態**のとり得る集合（迷路だとプレイヤーのとり得るすべての座標\\{`(x, y)`\\}）<br/> $\\quad S=\\{s\\}$<br/>\n",
    " - **報酬**の集合（迷路だと\\{`0, 1`\\} = \\{解けてない, 解けた \\}）<br/> $\\quad R=\\{r\\}$<br/>\n",
    " - **行動**の集合（迷路だと\\{`0, 1, 2, 3` \\} = \\{↑, ↓, ←, → \\}）<br/> $\\quad A=\\{a\\}$\n",
    "\n",
    "- **環境**の持つ性質 (実装上は`Env.step(a)`で同時に計算)\n",
    " - $s_{t+1} = \\text{step}_0(s_t, a_t)$\n",
    " - $r_{t+1} = \\text{step}_1(s_t, a_t, s_{t+1})$\n",
    "\n",
    "### ● より一般の環境について\n",
    "\n",
    "上に書いた$\\text{step}_{0, 1}$は関数なので、入力値が定まれば出力値は確定しています。\n",
    "しかし、一般にはこれらが確定していない場合もあります。\n",
    "> **【補足】** 例えば囲碁の盤面の**状態**とその時に置いた碁石の位置（**行動**）が何らかの具体的な値$(s_t, a_t)$を取ったからと言って、相手がどう出るかわからないので、次の自分の番での**状態** $s_{t+1}$が確定しているわけではありません。\n",
    "\n",
    "このような場合も考慮に入れるために、確率的な定式化を導入します。P(x)から実際に値をサンプリングすることを\n",
    "\n",
    "$$ x \\sim P(x) $$\n",
    "\n",
    "と書くことにすると、$P_s, P_r$をそれぞれ状態と報酬が与えられる確率だとして、\n",
    "- **環境**の持つ性質(一般)\n",
    " - $s_{t+1} \\sim P_s(s_{t+1}|s_t, a_t)$\n",
    " - $r_{t+1} \\sim P_r(r_{t+1}|s_t, a_t, s_{t+1})$\n",
    "\n",
    "と書けます。迷路のように決定している場合はデルタ関数などで表現できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1-2. エージェント\n",
    "ここまでは手で操作してきました。つまり\n",
    "* エージェント＝あなた自身\n",
    "\n",
    "だったわけです。\n",
    "\n",
    "あなた自身が迷路ゲームをプレイするとき、気分によって同じ座標に居ても↑だったり↓だったり選択するので、ゲームのプレイ方針は確率的といえるでしょう。このような「エージェントが持っているゲームのプレイ方針を記述する確率」を**方策(Policy)**といいます。\n",
    "\n",
    "あなた自身の何らかの**方策**に基づいてエージェントを操作していたわけですが、強化学習ではその部分を機械に置き換えたいわけです。そうすると、機械のエージェントの実装に必要なのは**方策** と、それに従うゲームのプレイ＝**行動**のサンプリング、ですから"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, Policy):\n",
    "        self.Policy = Policy\n",
    "        \n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "        return a number in [0,1,2,3] corresponding to [up, down, left, right]\n",
    "        \"\"\"\n",
    "        return self.Policy.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "のような実装になるでしょう。ここで**方策**も何らかの条件付き確率で与えられることを前提としています：\n",
    "\n",
    "- エージェントが持つべき性質\n",
    " - **方策**をあらわす条件付き確率<br/> $\\quad \\pi(a_t|s_t)$<br/>\n",
    " - そこからのサンプリング<br/> $\\quad a_t \\sim \\pi(a_t|s_t)$<br/>\n",
    "\n",
    "\n",
    "**`Policy`** はこの確率を記述するオブジェクトであり、**`Policy.sample()`**はサンプリングを記述するものです。\n",
    "\n",
    "従って **`Policy`** は **`sample()`** 関数を持った何らかのオブジェクトとして"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        プロトタイプなので pass とか適当でいいですが、後に\n",
    "        [0,1,2,3] = [up, down, left, right] から一つ数を返す用に実装\n",
    "        \"\"\"\n",
    "        action = None\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "のようなものを想定しています。たとえば、完全にランダムな方策\n",
    "\n",
    "$$\n",
    "\\pi_\\text{random}(a|s)\n",
    "=\n",
    "\\frac{1}{|A|},\n",
    "\\quad\n",
    "A = \\{a\\}\n",
    "$$\n",
    "\n",
    "は"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Random(Policy):\n",
    "    \"\"\"書いてください\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "のように書けます。実際にこの方策を用いてゲームを1回プレイさせてみます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Agt = Agent(Policy=Random(Env)) # Random方策に基づく機械エージェント\n",
    "Env.reset()\n",
    "Env.render()\n",
    "\n",
    "action = Agt.play()\n",
    "print(a2m[action])\n",
    "Env.step(action)\n",
    "Env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1-3. マルコフ決定過程\n",
    "\n",
    "ここまでで\n",
    "* **環境**：$\\{ P_s(s_{t+1}|s_t, a_t), \\ P_r(r_{t+1}|s_t, a_t, s_{t+1})\\}$ = \\{**状態**の時間発展,  **即時報酬**の時間発展 \\}\n",
    "* **エージェント**：$\\{ \\pi(a_t|s_t)\\}$ = \\{ **行動**の時間発展 \\}\n",
    "\n",
    "と3種類の確率変数$\\{ s, r, a\\}$についての時間発展を定義してきました。強化学習では、この3種類の確率変数の時間発展をゲームが終わるまで行います：\n",
    "\n",
    "$$\n",
    "\\left. \\begin{array}{ll:ll:ll:ll}\n",
    "s_0 \n",
    "\\overset{\\pi(\\cdot|s_0)}{\\to}\n",
    "&\n",
    "a_0 \n",
    "\\overset{P_s(\\cdot|s_0, a_0)}{\\to} \n",
    "&\n",
    "s_1 \n",
    "&\n",
    "\\overset{\\pi(\\cdot|s_1)}{\\to}\n",
    "a_1  \n",
    "\\overset{P_s(\\cdot|s_1, a_1)}{\\to} \n",
    "&\n",
    "s_2\n",
    "&\n",
    "\\overset{\\pi(\\cdot|s_2)}{\\to}\n",
    "a_2\n",
    "\\overset{P_s(\\cdot|s_2, a_2)}{\\to} \n",
    "&\n",
    " \\cdots\n",
    "\\\\\n",
    "\\downarrow_{P_r(\\cdot|-, -, s_0)} \n",
    "&&\n",
    "\\downarrow_{P_r(\\cdot|s_0, a_0, s_1)} \n",
    "&&\n",
    "\\downarrow_{P_r(\\cdot|s_1, a_1, s_2)} \n",
    "\\\\\n",
    "r_0\n",
    "&\n",
    "&\n",
    "r_1\n",
    "&\n",
    "&\n",
    "r_2\n",
    "\\end{array} \\right.\n",
    "\\tag{1.3}\n",
    "$$\n",
    "\n",
    "これを**マルコフ決定過程(Markov Decision Process, MDP)**といいます。\n",
    "\n",
    "\n",
    "ゲームが始まってから終わるまでの1単位（**MDP**の1つのサンプル系列）を**エピソード(episode)**と呼びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化学習の理論的な部分は主にこの**MDP**に基づいた確率論で記述されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 迷路のMDPから1エピソードサンプル再生\n",
    "Agt = Agent(Policy=Random(Env)) # Random方策に基づく機械エージェント\n",
    "Env.reset()\n",
    "%matplotlib notebook \n",
    "Env.play_interactive(Agt)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
