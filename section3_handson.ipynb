{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('bmh')\n",
    "%matplotlib inline\n",
    "\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container {width:100% !important;}</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# section1, 2 で実装したもので必要なもの\n",
    "from maze import MazeEnv\n",
    "from agents import Agent\n",
    "from policies import Policy, Parameters\n",
    "from optimizers import Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 方策最適化に基づく学習アルゴリズム\n",
    "\n",
    "価値ベースの方法は、「行動空間」`Env.action_space` が巨大すぎる場合などではトレーニングに失敗することがあります。別の選択肢の 1 つは、値 Q(s,a) ではなく、ポリシー $\\pi$ 自体のモデルを作成することです。\n",
    "\n",
    "## 3-1. 状態価値関数、期待収益、方策勾配法\n",
    "[Section2](https://github.com/AkinoriTanaka-phys/HPC-Phys_tutorial_and_hands-on/blob/master/section2.ipynb)\n",
    "では行動価値関数を\n",
    "\n",
    "$$\n",
    "Q^\\pi(s,a) = \\langle \\sum_{k=0}^\\infty \\gamma^k r_{t+1+k} \\rangle_{(s_t, a_t)=(s,a)}\n",
    "$$\n",
    "\n",
    "としました。これは状態 s で行動 a を取った時の収益の期待値だったわけですが、これを更に a について方策で期待値を取ったもの\n",
    "\n",
    "$$\n",
    "V^{\\pi}(s) = \\langle Q^{\\pi}(s, a) \\rangle_{a \\sim \\pi(\\cdot|s)}\n",
    "$$\n",
    "\n",
    "これは「方策 $\\pi$ のもとで状態 s が持っている価値」を表し、**状態価値関数(state value function)** と呼ばれます。更に、迷路の実装を思い出すと"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Env = MazeEnv(5,5, 1.2, figsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Env.reset() # 実行のたびにエージェントの位置が変わる\n",
    "Env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "の**`Env.reset()`**は最初の状態 s をランダムに選んでいるのがわかります。この確率を $P_\\text{reset}(s)$ とし、状態価値の期待値を取ると\n",
    "\n",
    "$$\n",
    "J(\\pi) = \\langle V^{\\pi}(s) \\rangle_{s \\sim P_\\text{reset} (\\cdot)}\n",
    "$$\n",
    "\n",
    "となりますが、環境が固定されている以上この量は「方策にしか依存しない量」であり、かつ方策の良さを表す量（大きいほど良い）であると言えるでしょう。これを**期待収益(expected return)**といいます。\n",
    "\n",
    "### ● 方策勾配法\n",
    "そこで、価値の推定をすっ飛ばして、この$J(\\pi)$を直接最大化する学習が考えられます。そのうちでも最も単純なのが\n",
    "方策を θ でパラメーター化し\n",
    "\n",
    "$$\n",
    "\\pi(a|s) = \\pi_\\theta(a|s)\n",
    "$$\n",
    "\n",
    "この時の期待収益を $\\theta$ の関数と考え、勾配更新\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\eta \\nabla_\\theta J(\\pi_{\\theta})\n",
    "$$\n",
    "\n",
    "によって学習を進めてゆくことです。これを**方策勾配法(Policy-gradient method)**といいます。\n",
    "\n",
    "問題は方策勾配 $\\nabla_\\theta J(\\pi_{\\theta})$ をどうやって求めるのか、というところです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3-2. 方策勾配定理とREINFORCEアルゴリズム\n",
    "ここで少し定義を変更させてください。今までは行動価値観数を\n",
    "\n",
    "$$\n",
    "Q^\\pi(s,a) = \\langle \\sum_{k=0}^\\infty \\gamma^k r_{t+1+k} \\rangle_{(s_t, a_t)=(s,a)}\n",
    "$$\n",
    "\n",
    "と減衰和にしていましたが、ここからは\n",
    "\n",
    "$$\n",
    "Q^\\pi(s,a) = \n",
    "\\lim_{T \\to \\infty}\n",
    "\\langle \\frac{1}{T}\\sum_{k=0}^{T-1}  r_{t+1+k} \\rangle_{(s_t, a_t)=(s,a)}\n",
    "$$\n",
    "\n",
    "と平均和の時間無限大極限に取ることにします。この場合のベルマン方程式は、\n",
    "\n",
    "$$\n",
    "Q^\\pi(s,a) = \\langle\n",
    "Q^\\pi(s_{+1}, a_{+1})\n",
    "\\rangle_{s_{+1} \\sim P_s(\\cdot|s, a),\\ a_{+1} \\sim \\pi(\\cdot | s_{+1})}\n",
    "$$\n",
    "\n",
    "となります。これを用いて方策勾配を求めてみましょう。まず状態価値観数の方策勾配から求めると\n",
    "\n",
    "$$\n",
    "\\left. \\begin{array}{rl}\n",
    "\\nabla_\\theta \\underbrace{ V^{\\pi_\\theta}(s) }_{\\langle Q^{\\pi_\\theta} (s, a) \\rangle_{a \\sim \\pi_\\theta(\\cdot|s)}}\n",
    "&=\n",
    "\\nabla_\\theta \n",
    "\\sum_{a}\n",
    "Q^{\\pi_\\theta} (s, a)\n",
    "\\pi_\\theta(a|s)\n",
    "\\\\\n",
    "&=\n",
    "\\sum_{a}\n",
    "\\Big(\n",
    "\\underbrace{\n",
    "\\nabla_\\theta \n",
    "Q^{\\pi_\\theta} (s, a)\n",
    "}_{(*)}\n",
    "\\cdot\n",
    "\\pi_\\theta(a|s)\n",
    "+\n",
    "Q^{\\pi_\\theta} (s, a)\n",
    "\\cdot\n",
    "\\underbrace{\n",
    "\\nabla_\\theta \n",
    "\\pi_\\theta(a| s)\n",
    "}_{\\big(\\nabla_\\theta \\log \\pi_\\theta(a | s)\\big) \\cdot \\pi_\\theta(a | s)}\n",
    "\\Big)\n",
    "\\\\\n",
    "&=\n",
    "\\sum_{a}\n",
    "\\Big(\n",
    "(*)\n",
    "+\n",
    "Q^{\\pi_\\theta} (s, a)\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a | s)\n",
    "\\Big) \\cdot\n",
    "\\pi_\\theta(a|s)\n",
    "\\\\\n",
    "&=\n",
    "\\big\\langle\n",
    "(*)\n",
    "+\n",
    "Q^{\\pi_\\theta} (s, a)\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a | s)\n",
    "\\big\\rangle_{a \\sim \\pi_\\theta(\\cdot|s)}\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "となって、行動価値関数の方策勾配を計算する必要に迫られます。しかしこれはベルマン方程式のため、再帰的に計算でき"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left. \\begin{array}{rl}\n",
    "(*) &= \n",
    "\\nabla_\\theta \n",
    "\\underbrace{Q^{\\pi_\\theta} (s, a)}_{Bellman}\n",
    "\\\\\n",
    "&=\n",
    "\\nabla_\\theta \n",
    "\\langle\n",
    "Q^{\\pi_\\theta}(s_{+1}, a_{+1})\n",
    "\\rangle_{s_{+1} \\sim P_s(\\cdot|s, a),\\ a_{+1} \\sim \\pi_\\theta(\\cdot | s_{+1})}\n",
    "\\\\\n",
    "&=\n",
    "\\nabla_\\theta \n",
    "\\sum_{s_{+1}, a_{+1}}\n",
    "Q^{\\pi_\\theta}(s_{+1}, a_{+1})\n",
    "\\pi_\\theta(a_{+1} | s_{+1})\n",
    "P_s(s_{+1}|s, a)\n",
    "\\\\\n",
    "&=\n",
    "\\sum_{s_{+1}, a_{+1}}\n",
    "\\Big(\n",
    "\\underbrace{\n",
    "\\nabla_\\theta \n",
    "Q^{\\pi_\\theta}(s_{+1}, a_{+1})\n",
    "}_{(*)}\n",
    "\\cdot\n",
    "\\pi_\\theta(a_{+1} | s_{+1})\n",
    "+\n",
    "Q^{\\pi_\\theta}(s_{+1}, a_{+1})\n",
    "\\cdot\n",
    "\\underbrace{\n",
    "\\nabla_\\theta \n",
    "\\pi_\\theta(a_{+1} | s_{+1})\n",
    "}_{\\big(\\nabla_\\theta \\log \\pi_\\theta(a_{+1} | s_{+1})\\big) \\cdot \\pi_\\theta(a_{+1} | s_{+1})}\n",
    "\\Big)\n",
    "P_s(s_{+1}|s, a)\n",
    "\\\\\n",
    "&=\n",
    "\\sum_{s_{+1}, a_{+1}}\n",
    "\\Big(\n",
    "(*)\n",
    "+\n",
    "Q^{\\pi_\\theta}(s_{+1}, a_{+1})\\nabla_\\theta \\log \\pi_\\theta(a_{+1} | s_{+1})\n",
    "\\Big) \\cdot\n",
    "\\pi_\\theta(a_{+1} | s_{+1})\n",
    "P_s(s_{+1}|s, a)\n",
    "\\\\\n",
    "&=\n",
    "\\big\\langle\n",
    "(*)\n",
    "+\n",
    "Q^{\\pi_\\theta}(s_{+1}, a_{+1})\\nabla_\\theta \\log \\pi_\\theta(a_{+1} | s_{+1})\n",
    "\\big\\rangle_{a_{+1} \\sim \\pi_\\theta(\\cdot | s_{+1}),\\ s_{+1} \\sim P_s(\\cdot|s, a)}\n",
    "\\\\\n",
    "&=\n",
    "\\Big\\langle\n",
    "\\sum_{t=0}^\\infty\n",
    "Q^{\\pi_\\theta}(s_{t+1}, a_{t+1})\\nabla_\\theta \\log \\pi_\\theta(a_{t+1} | s_{t+1})\n",
    "\\Big\\rangle_{MDP,\\ (s_0, a_0)=(s, a)}\n",
    "\\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "と書け、(*)をもとの V の勾配に代入すると $a_t = a$ として\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta V(s)\n",
    "=\n",
    "\\Big\\langle\n",
    "\\sum_{t=0}^\\infty\n",
    "Q^{\\pi_\\theta}(s_{t}, a_{t})\\nabla_\\theta \\log \\pi_\\theta(a_{t} | s_{t})\n",
    "\\Big\\rangle_{MDP,\\ s_0=s}\n",
    "$$\n",
    "\n",
    "更に**期待収益** $J(\\pi_\\theta)$は、これの初期状態による期待値なので、結局\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\pi_\\theta)\n",
    "=\n",
    "\\Big\\langle\n",
    "\\sum_{t=0}^\\infty\n",
    "Q^{\\pi_\\theta}(s_{t}, a_{t})\\nabla_\\theta \\log \\pi_\\theta(a_{t} | s_{t})\n",
    "\\Big\\rangle_{MDP}\n",
    "$$\n",
    "\n",
    "が得られます。これを**方策勾配定理(Policy-gradient theorem)**と言います。\n",
    "\n",
    "### ● REINFORCEアルゴリズム\n",
    "実際には、期待値はサンプル$(r_t, s_t, a_t)$によって近似され\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\pi_\\theta)\n",
    "\\approx\n",
    "\\sum_{t=0}^\\infty\n",
    "Q^{\\pi_\\theta}(s_{t}, a_{t})\\nabla_\\theta \\log \\pi_\\theta(a_{t} | s_{t})\n",
    "$$\n",
    "\n",
    "とされますが、ここで結局 $Q^{\\pi_\\theta}(s_{t}, a_{t})$ が必要になってしまいました。この行動価値関数の意味は、収益の期待値だったわけですので、これを更に報酬のサンプルで適当な T (典型的にはサンプルのエピソード長さが使われる)\n",
    "\n",
    "$$\n",
    "Q^{\\pi_\\theta}(s_{t}, a_{t})\n",
    "\\approx\n",
    "\\frac{1}{T} \\sum_{k=0}^{T-1} r_{t+1+k}\n",
    "$$\n",
    "\n",
    "で近似することが考えられます。これを**REINFORCEアルゴリズム**と言います。すなわち、REINFORCEアルゴリズムとは\n",
    "1. MDPサンプル$\\{(r_t, s_t, a_t)\\}_{t=0, 2, \\dots, T-1}$を得る\n",
    "2. $\n",
    "\\theta \\leftarrow \\theta + \\eta g_\\theta,\\quad \\text{where} \\\n",
    "g_\\theta = \\sum_{t=0}^\\infty\n",
    "\\Big(\n",
    "\\frac{1}{T} \\sum_{k=0}^{T-1} r_{t+1+k}\n",
    "\\Big)\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a_{t} | s_{t})\n",
    "$\n",
    "\n",
    "を繰り返して、良い方策$\\pi_\\theta$を探索する手法のことです。\n",
    "> **【補足】** MDPサンプルは一回の更新につきなるべく多く取ったほうが方策勾配$g_t$の精度が上がります。その場合はエピソード数$m = 0, 1, \\dots, M-1$として$g_\\theta = \n",
    "\\frac{1}{M} \\sum_{m=0}^{M-1}\n",
    "\\sum_{t=0}^\\infty\n",
    "\\Big(\n",
    "\\frac{1}{T} \\sum_{k=0}^{T-1} r_{t+1+k}^{(m)}\n",
    "\\Big)\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a_{t}^{(m)} | s_{t}^{(m)})$です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3-3. Softmax方策による実装\n",
    "\n",
    "実際にREINFORCEを実装するにあたって、**Softmax方策**というのを導入します。これは適当な関数 $f(s, a)$ を用いて\n",
    "\n",
    "$$\n",
    "\\pi_f(a|s)\n",
    "=\n",
    "\\frac{\\exp\\{f(s,a)\\}}{\\sum_{a'} \\exp\\{\n",
    "f(s, a')\n",
    "\\}}\n",
    "$$\n",
    "\n",
    "とする方策です。[Section2](https://github.com/AkinoriTanaka-phys/HPC-Phys_tutorial_and_hands-on/blob/master/section2.ipynb)と同様に、$f(s,a)$をパラメータとして"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Env = MazeEnv(5,5, figsize=3)\n",
    "f = Parameters(Env)\n",
    "Env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "と実装してみます。Softmax方策を実装するにあたって、まずsoftmax関数\n",
    "\n",
    "$$\n",
    "\\text{softmax}\\Big(\n",
    "\\begin{pmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "\\end{pmatrix}\n",
    "\\Big)\n",
    "=\n",
    "\\frac{1}{\n",
    "\\exp\\{x_0\\}+\\exp\\{x_1\\}+\\exp\\{x_2\\}+\\exp\\{x_3\\}\n",
    "}\n",
    "\\begin{pmatrix}\n",
    "\\exp\\{x_0\\} \\\\\n",
    "\\exp\\{x_1\\} \\\\\n",
    "\\exp\\{x_2\\} \\\\\n",
    "\\exp\\{x_3\\} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "を実装しておきます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(xs):\n",
    "    \"\"\"書いてください\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こうしておいて、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Softmax(Policy):\n",
    "    def __init__(self, Env, f=None, temp=1):\n",
    "        \"\"\"書いてください\"\"\"\n",
    "        \n",
    "    def get_prob_table(self):\n",
    "        \"\"\"書いてください\"\"\"\n",
    "    \n",
    "    def get_prob(self, state):\n",
    "        \"\"\"書いてください\"\"\"\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"書いてください\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "で良いでしょう。（実装では**温度**`temp`を導入して確率をコントロールできるようにしました）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ● 迷路の場合のREINFORCEアルゴリズム\n",
    "迷路の環境では、時刻$t$で\n",
    "* ゴールしていれば$r_t = 1$\n",
    "* していなければ$r_t=0$\n",
    "\n",
    "だったので、1エピソードで共通して方策勾配は\n",
    "\n",
    "$$\n",
    "g_\\theta\n",
    "=\n",
    "\\left\\{ \\begin{array}{ll}\n",
    "\\frac{1}{T} \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta (a_t|s_t) & (\\text{if solved}) \\\\\n",
    "0 & (\\text{if unsolved}) \\\\\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "ということになります。この方策勾配はSoftmax方策だともう少し簡単化出来ます。ゴールした場合の勾配にSoftmaxを$\\theta=f(s,a)$として代入してみると\n",
    "\n",
    "$$\n",
    "\\left. \\begin{array}{ll}\n",
    "\\frac{1}{T}\n",
    "\\sum_{t=0}^{T-1} \\nabla_{f(s,a)} \\log \\pi_f (a_t|s_t) \n",
    "&=\n",
    "\\frac{1}{T}\n",
    "\\sum_{t=0}^{T-1} \\nabla_{f(s,a)} \\log \n",
    "\\frac{\\exp\\{f(s_t,a_t)\\}}{\\sum_{a'} \\exp\\{\n",
    "f(s_t, a')\n",
    "\\}}\n",
    "\\\\\n",
    "&=\n",
    "\\frac{1}{T}\n",
    "\\sum_{t=0}^{T-1} \\nabla_{f(s,a)} \n",
    "\\Big(\n",
    "f(s_t,a_t)\n",
    "-\n",
    "\\log \\sum_{a'} \\exp\\{\n",
    "f(s_t, a')\n",
    "\\}\n",
    "\\Big)\n",
    "\\\\\n",
    "&=\n",
    "\\frac{1}{T}\n",
    "\\sum_{t=0}^{T-1} \n",
    "\\Big(\n",
    "\\overbrace{ \\nabla_{f(s,a)} f(s_t, a_t)}^{\\delta_{(s,a)=(s_t, a_t)}}\n",
    "-\n",
    "\\frac{\n",
    "\\sum_{a''}\n",
    "\\overbrace{ \\nabla_{f(s,a)} f(s_t, a'')}^{\\delta_{(s,a)=(s_t, a'')}}\n",
    "\\cdot\n",
    "\\exp\\{\n",
    "f(s_t, a'')\n",
    "\\}\n",
    "}{\n",
    "\\sum_{a'} \\exp\\{\n",
    "f(s_t, a')\n",
    "\\}\n",
    "}\n",
    "\\Big)\n",
    "\\\\\n",
    "&=\n",
    "\\frac{1}{T}\n",
    "\\sum_{t=0}^{T-1} \n",
    "\\Big(\n",
    "\\delta_{(s,a)=(s_t, a_t)}\n",
    "-\n",
    "\\delta_{s=s_t}\n",
    "\\underbrace{\n",
    "\\frac{\n",
    "\\exp\\{\n",
    "f(s_t, a)\n",
    "\\}\n",
    "}{\n",
    "\\sum_{a'} \\exp\\{\n",
    "f(s_t, a')\n",
    "\\}\n",
    "}\n",
    "}_{\\pi_f(a|s_t)}\n",
    "\\Big)\n",
    "\\\\\n",
    "&=\n",
    "\\frac{1}{T}\\Big(\n",
    "N_{(s,a)}\n",
    "-\n",
    "N_s \\pi_f(a|s)\n",
    "\\Big)\n",
    "\\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "となるのがわかります。ここで\n",
    "* $N_{(s,a)}$:エピソード中に状態 s で行動 a を選んだ回数\n",
    "* $N_s$:エピソード中に状態 s を取った回数\n",
    "\n",
    "を表すとします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class REINFORCE_optimizer(Optimizer):\n",
    "    def __init__(self, Agt, eta):\n",
    "        \"\"\"書いてください\"\"\"\n",
    "            \n",
    "    def update(self, s, a, r_next, s_next):\n",
    "        \"\"\"書いてください\"\"\"\n",
    "            \n",
    "    def reset(self):\n",
    "        \"\"\"書いてください\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Env = MazeEnv(15,15, 1.2)\n",
    "Env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "f = Parameters(Env)\n",
    "Pi=Softmax(Env, f=f, temp=1)\n",
    "Agt = Agent(Pi)\n",
    "Opt = REINFORCE_optimizer(Agt, eta=10)\n",
    "N_episode = 600 # 600でも十分\n",
    "\n",
    "for episode in range(N_episode):\n",
    "    \"\"\"書いてください\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Opt.N # 解けた回数＝実際の更新回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Env.reset()\n",
    "%matplotlib notebook \n",
    "Env.play_interactive(Agt)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "Env.reset()\n",
    "Env.render(values_table=Pi.get_prob_table())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
