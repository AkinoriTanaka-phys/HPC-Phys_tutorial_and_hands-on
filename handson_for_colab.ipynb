{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "handson_for_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/iuAP8m0ipgEn0q7lRTn4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkinoriTanaka-phys/HPC-Phys_tutorial_and_hands-on/blob/master/handson_for_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBmIJBOf-fVD",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# 1. マルコフ決定過程\n",
        "---\n",
        "### 1-1. 環境\n",
        "強化学習は教師あり学習とは異なり、データを用いません。\n",
        "代わりに**環境(Environment)**が与えられると考えます。\n",
        "今回は事前に以下のセルを実行すれば読み込まれる「迷路」の環境を用いて説明します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zn5U1kY984u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('bmh')\n",
        "\n",
        "import matplotlib.collections as mc\n",
        "import copy\n",
        "\n",
        "action2vect = {0: np.array([0, -1]),\n",
        "               1: np.array([0, +1]),\n",
        "               2: np.array([-1, 0]),\n",
        "               3: np.array([+1, 0])\n",
        "               }\n",
        "\n",
        "a2m = {0:'up', 1:'down', 2:'left', 3:'right'}\n",
        "\n",
        "def random_initialize(Maze):\n",
        "    floor_labels = np.arange(len(Maze.floors))\n",
        "    start_floor_label = np.random.choice(floor_labels)\n",
        "    goal_floor_label = np.random.choice(floor_labels)\n",
        "    #Maze.set_start(Maze.floors[start_floor_label].tolist())\n",
        "    Maze.set_goal(Maze.floors[goal_floor_label].tolist())\n",
        "    return Maze\n",
        "\n",
        "def get_fig_ax(size=(8, 5)):\n",
        "    fig = plt.figure(figsize=size)\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    return fig, ax\n",
        "        \n",
        "class MazeEnv():\n",
        "    def __init__(self, lx, ly, threshold=0.9, figsize=5):\n",
        "        self.lx = lx\n",
        "        self.ly = ly\n",
        "        self.create_maze_by_normal_distribution(threshold=threshold)\n",
        "        self = random_initialize(self)\n",
        "        \n",
        "        self.action_space = [0,1,2,3]\n",
        "        self.status = 'Initialized'\n",
        "        self.figsize = figsize\n",
        "        \n",
        "    def reset(self, coordinate=[None, None]):\n",
        "        \"\"\"\n",
        "        put the state at the start.\n",
        "        \"\"\"\n",
        "        if coordinate[0]!=None:\n",
        "            self.state = np.array(coordinate)\n",
        "        else:\n",
        "            #\n",
        "            floor_labels = np.arange(len(self.floors))\n",
        "            start_floor_label = np.random.choice(floor_labels)\n",
        "            self.state = self.floors[start_floor_label]\n",
        "            #\n",
        "            #self.state = np.array(self.start)\n",
        "        self.status = 'Reset'\n",
        "        self.t = 0\n",
        "        return self.get_state()\n",
        "        \n",
        "    def is_solved(self):\n",
        "        \"\"\"\n",
        "        if the state is at the goal, returns True.\n",
        "        \"\"\"\n",
        "        return self.goal==self.state.tolist()\n",
        "    \n",
        "    def get_state(self):\n",
        "        \"\"\"\n",
        "        returns (x, y) coordinate of the state\n",
        "        \"\"\"\n",
        "        return copy.deepcopy(self.state)#, copy.deepcopy(self.state[1])\n",
        "            \n",
        "    def step0(self, state, action):\n",
        "        add_vector_np = action2vect[action]\n",
        "        if (state+add_vector_np).tolist() in self.floors.tolist():\n",
        "            next_state = state+add_vector_np\n",
        "            self.status = 'Moved'\n",
        "        else:\n",
        "            next_state = state\n",
        "            self.status = 'Move failed'\n",
        "        self.t += 1\n",
        "        return next_state\n",
        "    \n",
        "    def step1(self, state, action, state_p):\n",
        "        if state_p.tolist()==self.goal:\n",
        "            reward = 1\n",
        "        elif False:\n",
        "            reward = 0.1\n",
        "        else:\n",
        "            reward = 0\n",
        "        return reward\n",
        "    \n",
        "    def step(self, action):\n",
        "        state = self.get_state()\n",
        "        next_state = self.step0(state, action)\n",
        "        reward = self.step1(state, action, next_state)\n",
        "        # self.state update\n",
        "        self.state = next_state\n",
        "        return self.get_state(), reward, self.is_solved(), {}\n",
        "        \n",
        "    def create_maze_by_normal_distribution(self, threshold):\n",
        "        \"\"\"\n",
        "        creating a random maze.\n",
        "        Higher threshold creates easier maze.\n",
        "        around threshold=1 is recomended.\n",
        "        \"\"\"\n",
        "        x = np.random.randn(self.lx*self.ly).reshape(self.lx, self.ly)\n",
        "        y = (x < threshold)*(x > -threshold)\n",
        "        self.tile = y\n",
        "        self.load_tile()\n",
        "        \n",
        "    def load_tile(self):\n",
        "        self.floors = np.array(list(np.where(self.tile==True))).T # (#white tiles, 2), 2 means (x,y) coordinate\n",
        "        self.holes = np.array(list(np.where(self.tile==True))).T # (#black tiles, 2)\n",
        "\n",
        "    def flip(self, coordinate=[None, None]):\n",
        "        self.tile[coordinate[0], coordinate[1]] = not self.tile[coordinate[0], coordinate[1]]\n",
        "        self.load_tile()\n",
        "    \n",
        "    def render_tile(self, ax, cmap='gray'):\n",
        "        ax.imshow(self.tile.T, interpolation=\"none\", cmap=cmap)\n",
        "        return ax\n",
        "    \n",
        "    def render_arrows(self, ax, values_table):\n",
        "        lx, ly, _ = values_table.shape\n",
        "        vmaxs = np.max(values_table, axis=2).reshape(lx, ly, 1)\n",
        "        vt = np.transpose(values_table*self.tile.reshape(lx, ly, 1)/vmaxs, (1,0,2))\n",
        "        width = 0.5\n",
        "        X, Y= np.meshgrid(np.arange(0, lx, 1), np.arange(0, ly, 1))\n",
        "        ones = .5*np.ones(lx*ly).reshape(lx, ly)\n",
        "        zeros= np.zeros(lx*ly).reshape(lx, ly)\n",
        "        # up\n",
        "        ax.quiver(X, Y, zeros, ones, vt[:,:,0], alpha=0.8, \n",
        "                      cmap='Reds', scale_units='xy', scale=1)\n",
        "        # down\n",
        "        ax.quiver(X, Y, zeros, -ones, vt[:,:,1], alpha=0.8, \n",
        "                      cmap='Reds', scale_units='xy', scale=1)\n",
        "        # left\n",
        "        ax.quiver(X, Y, -ones, zeros, vt[:,:,2], alpha=0.8, \n",
        "                      cmap='Reds', scale_units='xy', scale=1)\n",
        "        # right\n",
        "        ax.quiver(X, Y, ones, zeros, vt[:,:,3], alpha=0.8, \n",
        "                      cmap='Reds', scale_units='xy', scale=1)\n",
        "        return ax\n",
        "        \n",
        "    def render(self, fig=None, ax=None, lines=None, values_table=None):\n",
        "        canvas = False\n",
        "        if ax!=None:\n",
        "            pass\n",
        "            canvas = True\n",
        "            ax.clear()\n",
        "        else:\n",
        "            fig = plt.figure(figsize=(self.figsize, self.figsize))\n",
        "            ax = fig.add_subplot(111)\n",
        "            ax.set_xlabel('x')\n",
        "            ax.set_ylabel('y')\n",
        "        ####\n",
        "        ax = self.render_tile(ax)\n",
        "        \n",
        "        if values_table is not None:\n",
        "            ax = self.render_arrows(ax, values_table)\n",
        "        ####\n",
        "        try:\n",
        "            ax.scatter(self.start[0], self.start[1], marker='x', s=100, color='blue',\n",
        "                       alpha=0.8, label='start')\n",
        "        except AttributeError:\n",
        "            pass\n",
        "        try:\n",
        "            ax.scatter(self.goal[0], self.goal[1], marker='d', s=100, color='red',\n",
        "                       alpha=0.8, label='goal')\n",
        "        except AttributeError:\n",
        "            pass\n",
        "        try:\n",
        "            ax.scatter(self.state[0], self.state[1], marker='o', s=100, color='black',\n",
        "                       alpha=0.8, label='agent')\n",
        "        except AttributeError:\n",
        "            pass\n",
        "        if lines is not None:\n",
        "            lc = mc.LineCollection(lines, linewidths=2, color='black', alpha=0.5)\n",
        "            ax.add_collection(lc)\n",
        "        else:\n",
        "            pass\n",
        "            \n",
        "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left',\n",
        "                  scatterpoints=1)\n",
        "        if canvas:\n",
        "            #pass\n",
        "            fig.canvas.draw()\n",
        "        else:\n",
        "            plt.show()\n",
        "        \n",
        "    def set_start(self, coordinate=[None, None]):\n",
        "        if coordinate in self.floors.tolist():\n",
        "            self.start = coordinate\n",
        "        else:\n",
        "            print('Set the start on a white tile.')\n",
        "            \n",
        "    def set_goal(self, coordinate=[None, None]):\n",
        "        if coordinate in self.floors.tolist():\n",
        "            self.goal = coordinate\n",
        "        else:\n",
        "            print('Set the goal on a white tile.')\n",
        "                      \n",
        "    def play(self, Agent, show=True):\n",
        "        lines = []\n",
        "        while not self.is_solved():\n",
        "            state0 = self.get_state()\n",
        "            action = Agent.play()\n",
        "            self.step(action)\n",
        "            state1 = self.get_state()\n",
        "            lines.append([state0, state1])\n",
        "            if show:\n",
        "                self.render(lines=lines)\n",
        "                \n",
        "    def play_interactive(self, Agent):\n",
        "        fig = plt.figure(figsize=(8, 5))\n",
        "        ax = fig.add_subplot(111)\n",
        "        ax.set_xlabel('x')\n",
        "        ax.set_ylabel('y')\n",
        "        #self.render(fig=fig, ax=ax)\n",
        "        lines = []\n",
        "        while not self.is_solved():\n",
        "            state0 = self.get_state()\n",
        "            action = Agent.play()\n",
        "            self.step(action)\n",
        "            state1 = self.get_state()\n",
        "            lines.append([state0, state1])\n",
        "            #self.render(fig=fig, ax=ax, lines=lines)\n",
        "            #fig.canvas.draw()\n",
        "        self.render(fig=fig, ax=ax, lines=lines)\n",
        "        plt.show()\n",
        "        print(\"solved!\")\n",
        "\n",
        "\n",
        "class CliffEnv(MazeEnv):\n",
        "    def __init__(self, lx, ly, threshold=0.9, figsize=5):\n",
        "        self.lx = lx\n",
        "        self.ly = ly\n",
        "        self.create_cliff()\n",
        "        self.start = [0, ly-1]\n",
        "        self.goal = [lx-1, ly-1]\n",
        "        \n",
        "        self.action_space = [0,1,2,3]\n",
        "        self.status = 'Initialized'\n",
        "        self.figsize = figsize\n",
        "        \n",
        "    def reset(self, coordinate=[None, None]):\n",
        "        \"\"\"\n",
        "        put the state at the start.\n",
        "        \"\"\"\n",
        "        if coordinate[0]!=None:\n",
        "            self.state = np.array(coordinate)\n",
        "        else:\n",
        "            self.state = np.array(self.start)\n",
        "        self.status = 'Reset'\n",
        "        self.t = 0\n",
        "        return self.get_state()\n",
        "    \n",
        "    def create_cliff(self):\n",
        "        \"\"\"\n",
        "        creating a cliff\n",
        "        \"\"\"\n",
        "        x = np.ones(self.lx*self.ly).reshape(self.lx, self.ly)\n",
        "        x[:, self.ly-1] -= 1\n",
        "        x[0, self.ly-1] += 1\n",
        "        x[self.lx-1, self.ly-1] += 1\n",
        "        self.tile = x\n",
        "        self.load_tile()\n",
        "        \n",
        "    def render_tile(self, ax, cmap='Reds_r'):\n",
        "        ax.imshow(self.tile.T, interpolation=\"none\", cmap=cmap)\n",
        "        return ax\n",
        "    \n",
        "    def step0(self, state, action):\n",
        "        add_vector_np = action2vect[action]\n",
        "        if (state+add_vector_np).tolist() in self.floors.tolist():\n",
        "            next_state = state+add_vector_np\n",
        "            self.status = 'Moved'\n",
        "        elif (state+add_vector_np).tolist() in self.holes.tolist():\n",
        "            next_state = self.start\n",
        "            self.status = 'Dropped'\n",
        "        else:\n",
        "            next_state = state\n",
        "            self.status = 'Move failed'\n",
        "        self.t += 1\n",
        "        return next_state\n",
        "    \n",
        "    def step1(self, state, action, state_p):\n",
        "        if state_p.tolist()==self.goal:\n",
        "            reward = 1\n",
        "        elif self.status=='Dropped':\n",
        "            reward = -100\n",
        "        else:\n",
        "            reward = 0\n",
        "        return reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBDxysus-ryq",
        "colab_type": "text"
      },
      "source": [
        "まずは迷路の環境を読み込んでみましょう："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIf3OKXo-bxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"書いてください\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twUeqbbE-3LW",
        "colab_type": "text"
      },
      "source": [
        "**`Env.render()`**は環境を表示させる関数です。環境の関数名はなるべく<a href=\"https://gym.openai.com\">OpenAI Gym</a>を参考にしました。\n",
        "> **【補足】** <a href=\"https://gym.openai.com\">OpenAI Gym</a> はAtari社のブロック崩しゲームを始めとした、数々のゲームやその他の強化学習環境をpythonから呼び出すためのライブラリで、無料です。pipコマンドでダウンロードできます。\n",
        "\n",
        "ここで\n",
        "* ◾は通れない壁\n",
        "* ◆ は迷路のゴール地点\n",
        "\n",
        "を表すとします。早速この迷路のスタート地点に「プレイヤー」を置いてみましょう："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzPmY3W--y0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"書いてください\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdmVEUXU_Q6O",
        "colab_type": "text"
      },
      "source": [
        " ● が追加されました。これがプレイヤーの位置を表します。その座標(**状態(state)**といいます)は以下で確認できます："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yKU9HLT_P--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"書いてください\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq3dbcCa_Yxe",
        "colab_type": "text"
      },
      "source": [
        "プレイヤーは [↑、↓、←、→] を各座標で選択します。これを**行動(action)**と言います。行動のリストは：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrGGOFxi_VAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"書いてください\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VOOWDcc_frD",
        "colab_type": "text"
      },
      "source": [
        "後の便宜のため、[↑、↓、←、→] は `[0, 1, 2, 3]` で表現しています："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK0PY-28_eiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"書いてください\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pWRW7cn_uPC",
        "colab_type": "text"
      },
      "source": [
        "試しに ● を動かしてみましょう。それぞれ\n",
        "* `Env.step0(s, a)`：**状態:`s`**に居るときに**行動:`a`**を取ったときの次の**状態**を返す\n",
        "* `Env.step1(s, a, next_s)`：**状態:`s`**に居るときに**行動:`a`**を取り、**状態:`next_s`**に移った時の「**報酬(reward)**」の値を返す"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G169mKN_smt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"書いてください\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrBITYyP_6rD",
        "colab_type": "text"
      },
      "source": [
        "真ん中の3行を毎回書くのは面倒なので\n",
        "* `Env.step(a)`：上２つを同時に実行し、(**状態:`next_s`**, **報酬:`next_r`**, 解けたかどうか, 補足)を返す\n",
        "\n",
        "を用意しました："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o4_Z6mA_3_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"書いてください\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X14QgF6OAPtX",
        "colab_type": "text"
      },
      "source": [
        "### ● ここまでのまとめ\n",
        "- 登場する集合とその要素\n",
        " - **時刻**(状態、報酬、行動の添字) <br> $\\quad T=\\{0,1,2,3, \\dots\\}=\\{t\\}$<br/>\n",
        " - **状態**のとり得る集合（迷路だとプレイヤーのとり得るすべての座標\\{`(x, y)`\\}）<br/> $\\quad S=\\{s\\}$<br/>\n",
        " - **報酬**の集合（迷路だと\\{`0, 1`\\} = \\{解けてない, 解けた \\}）<br/> $\\quad R=\\{r\\}$<br/>\n",
        " - **行動**の集合（迷路だと\\{`0, 1, 2, 3` \\} = \\{↑, ↓, ←, → \\}）<br/> $\\quad A=\\{a\\}$\n",
        "\n",
        "- **環境**の持つ性質 (実装上は`Env.step(a)`で同時に計算)\n",
        " - $s_{t+1} = \\text{step}_0(s_t, a_t)$\n",
        " - $r_{t+1} = \\text{step}_1(s_t, a_t, s_{t+1})$\n",
        "\n",
        "### ● より一般の環境について\n",
        "\n",
        "上に書いた$\\text{step}_{0, 1}$は関数なので、入力値が定まれば出力値は確定しています。\n",
        "しかし、一般にはこれらが確定していない場合もあります。\n",
        "> **【補足】** 例えば囲碁の盤面の**状態**とその時に置いた碁石の位置（**行動**）が何らかの具体的な値$(s_t, a_t)$を取ったからと言って、相手がどう出るかわからないので、次の自分の番での**状態** $s_{t+1}$が確定しているわけではありません。\n",
        "\n",
        "このような場合も考慮に入れるために、確率的な定式化を導入します。P(x)から実際に値をサンプリングすることを\n",
        "\n",
        "$$ x \\sim P(x) $$\n",
        "\n",
        "と書くことにすると、$P_s, P_r$をそれぞれ状態と報酬が与えられる確率だとして、\n",
        "- **環境**の持つ性質(一般)\n",
        " - $s_{t+1} \\sim P_s(s_{t+1}|s_t, a_t)$\n",
        " - $r_{t+1} \\sim P_r(r_{t+1}|s_t, a_t, s_{t+1})$\n",
        "\n",
        "と書けます。迷路のように決定している場合はデルタ関数などで表現できます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldri6Vi1Akkt",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### 1-2. エージェント\n",
        "ここまでは手で操作してきました。つまり\n",
        "* エージェント＝あなた自身\n",
        "\n",
        "だったわけです。\n",
        "\n",
        "あなた自身が迷路ゲームをプレイするとき、気分によって同じ座標に居ても↑だったり↓だったり選択するので、ゲームのプレイ方針は確率的といえるでしょう。このような「エージェントが持っているゲームのプレイ方針を記述する確率」を**方策(Policy)**といいます。\n",
        "\n",
        "あなた自身の何らかの**方策**に基づいてエージェントを操作していたわけですが、強化学習ではその部分を機械に置き換えたいわけです。そうすると、機械のエージェントの実装に必要なのは**方策** と、それに従うゲームのプレイ＝**行動**のサンプリング、ですから"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0Zla5EPAM8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent():\n",
        "    def __init__(self, Policy):\n",
        "        self.Policy = Policy\n",
        "        \n",
        "    def play(self):\n",
        "        \"\"\"\n",
        "        return a number in [0,1,2,3] corresponding to [up, down, left, right]\n",
        "        \"\"\"\n",
        "        return self.Policy.sample()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wjllPfiArEw",
        "colab_type": "text"
      },
      "source": [
        "のような実装になるでしょう。ここで**方策**も何らかの条件付き確率で与えられることを前提としています：\n",
        "\n",
        "- エージェントが持つべき性質\n",
        " - **方策**をあらわす条件付き確率<br/> $\\quad \\pi(a_t|s_t)$<br/>\n",
        " - そこからのサンプリング<br/> $\\quad a_t \\sim \\pi(a_t|s_t)$<br/>\n",
        "\n",
        "\n",
        "**`Policy`** はこの確率を記述するオブジェクトであり、**`Policy.sample()`**はサンプリングを記述するものです。\n",
        "\n",
        "従って **`Policy`** は **`sample()`** 関数を持った何らかのオブジェクトとして"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HEcNzYpAqH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Policy():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def sample(self):\n",
        "        \"\"\"\n",
        "        プロトタイプなので pass とか適当でいいですが、後に\n",
        "        [0,1,2,3] = [up, down, left, right] から一つ数を返す用に実装\n",
        "        \"\"\"\n",
        "        action = None\n",
        "        return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gXZz3SCAzW4",
        "colab_type": "text"
      },
      "source": [
        "のようなものを想定しています。たとえば、完全にランダムな方策\n",
        "\n",
        "$$\n",
        "\\pi_\\text{random}(a|s)\n",
        "=\n",
        "\\frac{1}{|A|},\n",
        "\\quad\n",
        "A = \\{a\\}\n",
        "$$\n",
        "\n",
        "は"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1-xDbA5AyA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Random(Policy):\n",
        "    \"\"\"書いてください\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwhgY6PvA7zu",
        "colab_type": "text"
      },
      "source": [
        "のように書けます。実際にこの方策を用いてゲームを1回プレイさせてみます："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ8wbm0iA6nI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Agt = Agent(Policy=Random(Env)) # Random方策に基づく機械エージェント\n",
        "Env.reset()\n",
        "Env.render()\n",
        "\n",
        "action = Agt.play()\n",
        "print(a2m[action])\n",
        "Env.step(action)\n",
        "Env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h4JFrGzBGZF",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### 1-3. マルコフ決定過程\n",
        "\n",
        "ここまでで\n",
        "* **環境**：$\\{ P_s(s_{t+1}|s_t, a_t), \\ P_r(r_{t+1}|s_t, a_t, s_{t+1})\\}$ = \\{**状態**の時間発展,  **即時報酬**の時間発展 \\}\n",
        "* **エージェント**：$\\{ \\pi(a_t|s_t)\\}$ = \\{ **行動**の時間発展 \\}\n",
        "\n",
        "と3種類の確率変数$\\{ s, r, a\\}$についての時間発展を定義してきました。強化学習では、この3種類の確率変数の時間発展をゲームが終わるまで行います：\n",
        "\n",
        "$$\n",
        "\\left. \\begin{array}{ll:ll:ll:ll}\n",
        "s_0 \n",
        "\\overset{\\pi(\\cdot|s_0)}{\\to}\n",
        "&\n",
        "a_0 \n",
        "\\overset{P_s(\\cdot|s_0, a_0)}{\\to} \n",
        "&\n",
        "s_1 \n",
        "&\n",
        "\\overset{\\pi(\\cdot|s_1)}{\\to}\n",
        "a_1  \n",
        "\\overset{P_s(\\cdot|s_1, a_1)}{\\to} \n",
        "&\n",
        "s_2\n",
        "&\n",
        "\\overset{\\pi(\\cdot|s_2)}{\\to}\n",
        "a_2\n",
        "\\overset{P_s(\\cdot|s_2, a_2)}{\\to} \n",
        "&\n",
        " \\cdots\n",
        "\\\\\n",
        "\\downarrow_{P_r(\\cdot|-, -, s_0)} \n",
        "&&\n",
        "\\downarrow_{P_r(\\cdot|s_0, a_0, s_1)} \n",
        "&&\n",
        "\\downarrow_{P_r(\\cdot|s_1, a_1, s_2)} \n",
        "\\\\\n",
        "r_0\n",
        "&\n",
        "&\n",
        "r_1\n",
        "&\n",
        "&\n",
        "r_2\n",
        "\\end{array} \\right.\n",
        "$$\n",
        "\n",
        "これを**マルコフ決定過程(Markov Decision Process, MDP)**といいます。\n",
        "\n",
        "\n",
        "ゲームが始まってから終わるまでの1単位（**MDP**の1つのサンプル系列）を**エピソード(episode)**と呼びます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNVFhmj1BBcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Agt = Agent(Policy=Random(Env)) # Random方策に基づく機械エージェント\n",
        "Env.reset()\n",
        "Env.play_interactive(Agt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOS-zAUhBZU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUwCJN8kBfqt",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# 2. 価値推定に基づく学習アルゴリズム\n",
        "---\n",
        "## 2-1. 行動価値関数 $Q(s,a)$ と貪欲な方策\n",
        "方策として前節の**`Random`**でもゲームのプレイは出来ましたが、当然ムダな動きが多いので、\n",
        "* 各状態 s に対して「最適」な行動 a が存在するか？\n",
        "\n",
        "というのは自然な問いでしょう。素朴には「得られる即時報酬の総和」$\\sum_t r_t$ を最大化すれば良い気がしますが、よく取られるのは適当な減衰率 0<γ<1 を導入した、時刻t からの減衰和です：\n",
        "\n",
        "$$\n",
        "g_t = \\sum_{k=0}^\\infty \\gamma^k r_{t+1+k}\n",
        "$$\n",
        "\n",
        "これを**収益(return)**といいます。\n",
        "\n",
        "この値を MDP の系列の各時刻で最大にするようにしたいわけですが、$g_t, r_t$は確率変数なので、このままではゲームのプレイサンプル毎に値が変動してしまいます。\n",
        "\n",
        "### ● 行動価値関数 $Q(s,a)$\n",
        "そこでこれを、$P_s, P_r, \\pi$の確率から成る MDP における期待値で表すことにします。\n",
        "\n",
        "$$\n",
        "Q(s, a) = \\langle g_t \\rangle_{(s_t, a_t)=(s,a)}\n",
        "$$\n",
        "\n",
        "これを**行動価値関数(action value function)**と言います。\n",
        "MDPと並列して図示すると、\n",
        "\n",
        "$$\n",
        "\\left. \\begin{array}{l:r:ll:ll:l|l}\n",
        "\\dots\n",
        "&\n",
        "s, a\n",
        "{\\to} \n",
        "&\n",
        "s_{t+1}\n",
        "&\n",
        "\\overset{\\pi(\\cdot|s_{t+1})}{\\to}\n",
        "a_{t+1}\n",
        "{\\to} \n",
        "&\n",
        "s_{t+2}\n",
        "&\n",
        "\\overset{\\pi(\\cdot|s_{t+2})}{\\to}\n",
        "a_{t+2}\n",
        "{\\to} \n",
        "&\n",
        " \\cdots\n",
        "&\n",
        "\\\\\n",
        "&&\n",
        "\\downarrow\n",
        "&&\n",
        "\\downarrow\n",
        "\\\\\n",
        "&&\n",
        "r_{t+1}\n",
        "&&\n",
        "r_{t+2}\n",
        "\\\\ \\hline \n",
        "Q(s,a) =\n",
        "&\n",
        "\\langle\n",
        "&\n",
        "r_{t+1}\n",
        "&&\n",
        "+\\gamma r_{t+2}\n",
        "&&\n",
        "+ \\dots\n",
        "&\n",
        "\\rangle\n",
        "\\end{array} \\right.\n",
        "$$\n",
        "\n",
        "のように、時刻 t で s に居て、そこで a を選んだとして、以後ずっと同じ方策$\\pi$でゲームをプレイした時に得られる**収益**の期待値を表します。\n",
        "\n",
        "> **【補足】** Q(s,a)は方策$\\pi$に依存します\n",
        "\n",
        "$Q(s,a)$の値が今回の機械学習で推定するターゲットというわけです。そのため、学習パラメータのクラスを"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRSA7j9WBc9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Parameters():\n",
        "    def __init__(self, Env, init=0.01):\n",
        "        \"\"\"\n",
        "        書いてください\n",
        "        \"\"\"\n",
        "        \n",
        "    def get_values(self, s):\n",
        "        \"\"\"\n",
        "        書いてください\n",
        "        \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ehcR9YmB_dQ",
        "colab_type": "text"
      },
      "source": [
        "とでもしておきましょう。\n",
        "\n",
        "### ● greedy方策\n",
        "$Q(s,a)$が**`Parameters`**クラスとして実装できたとしたとして、$Q$の意味を思い出してみると、方策として\n",
        "\n",
        "$$\n",
        "\\pi_{greedy}(a|s)\n",
        "=\n",
        "\\delta \\Big(\n",
        "a - \n",
        "\\text{argmax}_{a'}\\big\\{ Q(s, a') \\big\\}\n",
        "\\Big)\n",
        "$$\n",
        "\n",
        "つまり、与えられた状態 s 毎に、行動価値観数が最大になる行動を取る方策を取れば良さそうです。これを**貪欲な方策(greedy policy)**と言います："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nys3NRwOB-je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Greedy(Policy):\n",
        "    def __init__(self, Env, Q=None):\n",
        "        \"\"\"書いてください\"\"\"\n",
        "        \n",
        "    def returns_action_from(self, values):\n",
        "        \"\"\"書いてください\"\"\"\n",
        "        \n",
        "    def sample(self):\n",
        "        \"\"\"書いてください\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK3_WYt5CKWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Pi = Greedy(Env=None, Q=None)\n",
        "action = Pi.returns_action_from(values=[0,100,9,10])\n",
        "action, a2m[action]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQxX2wlaCHus",
        "colab_type": "text"
      },
      "source": [
        "> **【補足】** np.argmax(リスト)は、リストに同じ値の最大値がある場合、一番添え字の若いものを選びます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixAV5UW5CQ0S",
        "colab_type": "text"
      },
      "source": [
        "### ● ε-greedy方策\n",
        "greedy方策には「遊び」がないため、局所解に囚われがちです。\n",
        "そこでこれを修正した**ε-貪欲方策(ε-greedy policy)**というものがよく使われます\n",
        "\n",
        "$$\n",
        "\\pi_{\\epsilon\\text{-}greedy}(a|s)\n",
        "=\n",
        "\\left\\{ \\begin{array}{ll}\n",
        "\\pi_{greedy}(a|s) & (\\text{w/ probability}\\quad 1- \\epsilon) \\\\\n",
        "\\pi_{random}(a|s) & (\\text{w/ probability}\\quad \\epsilon)\\\\\n",
        "\\end{array} \\right.\n",
        "$$\n",
        "\n",
        "> **【補足】** 強化学習における一つの大きな問題が「**探索と知識利用のトレードオフ(exploitation-exploration trade-offs)**」です。ε-greedy方策はεの値を調整することで、探索と知識利用の度合いを調整できます。\n",
        "\n",
        "Greedyとほぼ同じ実装で出来ますので、ここには書きませんが、あとで使いますので、\n",
        "\n",
        "以下を実行してください"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIjIe6_4CGoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EpsilonGreedy(Greedy):\n",
        "    def __init__(self, Env, Q, epsilon=0.1):\n",
        "        super(EpsilonGreedy, self).__init__(Env, Q) # python 3 : super().__init__(Env, Q)で十分\n",
        "        self.epsilon = epsilon\n",
        "    \n",
        "    def returns_action_from(self, values):\n",
        "        if np.random.rand()<1-self.epsilon:\n",
        "            action = np.argmax(values)\n",
        "        else:\n",
        "            action = np.random.choice(np.arange(len(values)))\n",
        "        return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcX9DrnhCaHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Pi = EpsilonGreedy(Env=None, Q=None, epsilon=0.8)\n",
        "a = Pi.returns_action_from(values=[0,100,9,10])\n",
        "a, a2m[a]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stgLBfaJCZHh",
        "colab_type": "text"
      },
      "source": [
        "### ● ここまでのまとめ\n",
        "* **`Parameters`** : $Q(s,a)$の推定値のためのクラス\n",
        "* **`Greedy(Policy)`** : 内部に`Parameters`インスタンスを持つgreedy方策\n",
        "* **`Agent`** : 内部に`Policy`インスタンスを持つエージェントのクラス\n",
        "\n",
        "を実装してきました。`Env`が迷路インスタンスだとして、\n",
        "```\n",
        "q = Parameters(Env)\n",
        "Agt = Agent(Policy=Greedy(Env=Env, Q=q))\n",
        "```\n",
        "とすれば、`Env`に対して、適当に初期化されたQの推定値qに基づくgreedyなエージェントが作れたことになります。\n",
        "\n",
        "### ● ここからやりたいこと\n",
        "この**`q=Parameters(Env)`**を$Q(s,a) = \\langle \\sum_{k=0}^\\infty \\gamma^k r_{t+1+k} \\rangle$\n",
        "に設定して迷路を解けるようにしたいわけですが、これを直接計算せず、ゲームプレイの経験から推定することを考えます。この推定を行うクラス\n",
        "* **`Optimizer`** : `Agent`を読み込み、学習させるクラス\n",
        "\n",
        "を実装し、学習は\n",
        "```\n",
        "Opt = Optimizer(Agt)\n",
        "...\n",
        "Opt.update()\n",
        "```\n",
        "のように進めるとします。`Optimizer`のプロトタイプは以下のようなものです："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bryxPUvCYHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Optimizer():\n",
        "    def __init__(self, Agt):\n",
        "        self.Agt = Agt\n",
        "    \n",
        "    def update(self):\n",
        "        \"\"\"\n",
        "        なんかいい感じの処理\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2DVnmP0DBOD",
        "colab_type": "text"
      },
      "source": [
        "以下では、この具体的な実装を考えていきます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82vxZMFhDC1M",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## 2-2. ベルマン方程式とSARSA\n",
        "\n",
        "### ● ベルマン方程式\n",
        "それを達成するために、$Q(s,q)$ がある漸化式を満たすことを示しましょう：\n",
        "\n",
        "$$\n",
        "\\left. \\begin{array}{ll}\n",
        "Q(s, a) \n",
        "&=\n",
        "\\langle \\underbrace{g_t}_\\text{substitute def} \\rangle_{(s_t, a_t)=(s,a)}\n",
        "\\\\&=\n",
        "\\langle \\underbrace{r_{t+1}}_{=:r_\\text{next}} + \\underbrace{\\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots}_{\\gamma g_{t+1}} \\rangle_{(s_t, a_t)=(s,a)}\n",
        "\\\\&=\n",
        "\\underbrace{ \\langle r_\\text{next} \\rangle_{(s_t, a_t)=(s,a)} }_\\text{only depends on $t$-th time evolution}\n",
        "+\n",
        "\\gamma\n",
        "\\underbrace{\\langle g_{t+1} \\rangle_{(s_t, a_t)=(s,a)}}_{\n",
        "\\big\\langle\n",
        "\\langle g_{t+1} \\rangle_{(s_{t+1}, a_{t+1})=(s_\\text{next}, a_\\text{next})}\n",
        "\\big \\rangle_{s_\\text{next} \\sim P_s(\\cdot|s, a),\\ a_\\text{next} \\sim \\pi(\\cdot | s_\\text{next})}\n",
        "}\n",
        "\\\\&=\n",
        "\\langle r_\\text{next} \\rangle_{s_\\text{next} \\sim P_s(\\cdot|s, a),\\ r_\\text{next} \\sim P_r(\\cdot|s, a, s_\\text{next})}\n",
        "+ \\gamma \\big\\langle\n",
        "\\underbrace{\\langle g_{t+1} \\rangle_{(s_{t+1}, a_{t+1})=(s_\\text{next}, a_\\text{next})}}_{Q(s_\\text{next}, a_\\text{next})}\n",
        "\\big \\rangle_{s_\\text{next} \\sim P_s(\\cdot|s, a),\\ a_\\text{next} \\sim \\pi(\\cdot | s_\\text{next})}\n",
        "\\\\&=\n",
        "\\langle r_\\text{next} \\rangle_{s_\\text{next} \\sim P_s(\\cdot|s, a),\\ r_\\text{next} \\sim P_r(\\cdot|s, a, s_\\text{next})}\n",
        "+ \\gamma \\langle\n",
        "Q(s_\\text{next}, a_\\text{next})\n",
        "\\rangle_{s_\\text{next} \\sim P_s(\\cdot|s, a),\\ a_\\text{next} \\sim \\pi(\\cdot | s_\\text{next})}\n",
        "\\end{array} \\right.\n",
        "$$\n",
        "\n",
        "これを**ベルマン方程式(Bellman equation)**と言います。\n",
        "\n",
        "> **【補足】** ここでは時間$t$は離散ですが、これを連続に拡張したほうが良い場合もあるでしょう（例えば実世界で動くエージェントの強化学習など）。連続時間への拡張は**ハミルトン-ヤコビ-ベルマン方程式(Hamilton-Jacobi-Bellman equation)**と呼ばれます。名前からわかるように古典力学におけるハミルトン-ヤコビ方程式の拡張になっているそうです。\n",
        "\n",
        "### ● SARSA\n",
        "ベルマン方程式の近似として、MDPのサンプル系列\n",
        "\n",
        "$$\n",
        "\\left. \\begin{array}{l:ll:ll:ll}\n",
        "\\dots\n",
        "&\n",
        "s\n",
        "&\n",
        "{\\to}\n",
        "a\n",
        "{\\to} \n",
        "&\n",
        "s_\\text{next}\n",
        "&\n",
        "{\\to}\n",
        "a_\\text{next}\n",
        "{\\to} \n",
        "&\n",
        " \\cdots\n",
        "\\\\\n",
        "&&&\n",
        "\\downarrow\n",
        "\\\\\n",
        "&\n",
        "-\n",
        "&&\n",
        "r_\\text{next}\n",
        "\\end{array} \\right.\n",
        "$$\n",
        "\n",
        "があったとき、期待値をサンプルで近似すると\n",
        "\n",
        "$$\n",
        "Q(s, a) = r_\\text{next} + \\gamma Q(s_\\text{next}, a_\\text{next})\n",
        "$$\n",
        "\n",
        "としても良さそうです。つまり$q(s,a)$をモデルとして「誤差関数」\n",
        "\n",
        "$$\n",
        "l(q) = \\frac{1}{2} \n",
        "\\Big(q(s,a) - \\big[ r_\\text{next} + \\gamma q(s_\\text{next}, a_\\text{next}) \\big] \\Big)^2\n",
        "$$\n",
        "\n",
        "を減らせば良さそうです。ニューラルネットの学習よろしく、勾配法でアップデートすることにすれば\n",
        "\n",
        "$$\n",
        "\\left. \\begin{array}{ll}\n",
        "q(s,a) &\\leftarrow \n",
        "q(s,a) - \\eta \\nabla_{q(s,a)} l(q)\n",
        "\\\\\n",
        "&= \n",
        "q(s,a) - \\eta \\underbrace{ \\Big(q(s,a) - \\big[ r_\\text{next} + \\gamma q(s_\\text{next}, a_\\text{next}) \\big] \\Big)}_\\text{TD error}\n",
        "\\end{array} \\right.\n",
        "$$\n",
        "\n",
        "を実際のMDPでのサンプル毎に更新すればよいでしょう。\n",
        "更新する分の部分を**Temporal Difference error(TD error)**と言います。\n",
        "TD errorの部分に順に$s, a, r_\\text{next}, s_\\text{next}, a_\\text{next}$が出てきていることから、この方法を**SARSA**といいます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPFEdh88C-m6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SARSA_optimizer(Optimizer):\n",
        "    def __init__(self, Agt, eta, gamma):\n",
        "        \"\"\"書いてください\"\"\"\n",
        "\n",
        "    def update(self, s, a, r_next, s_next):\n",
        "        \"\"\"書いてください\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iwHXFTaDlaY",
        "colab_type": "text"
      },
      "source": [
        "早速これで強化学習させてみます："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSgcCrk0DhCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Env = MazeEnv(10, 10, threshold=1.2)\n",
        "Env.reset()\n",
        "Env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgopsrb-Do1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "q = Parameters(Env)\n",
        "Agt = Agent(Policy=Greedy(Env=Env, Q=q))\n",
        "Opt = SARSA_optimizer(Agt, eta=1., gamma=0.2)\n",
        "N_episode = 6000\n",
        "\n",
        "for episode in range(N_episode):\n",
        "    \"\"\"書いてください\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PabEGMT1Dyky",
        "colab_type": "text"
      },
      "source": [
        "学習後のエージェントで迷路を解かせてみます："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MteggqUSDuUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Env.reset()\n",
        "Env.play_interactive(Agt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brLlZ4llEHOC",
        "colab_type": "text"
      },
      "source": [
        "`Env.render()`に推定された行動価値関数を図示するオプション\n",
        "* `Env.render(values_table)`\n",
        "\n",
        "を用意してみました。\n",
        "濃い色ほど$Q(s,a)$の値が大きいことを意味します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVKRwH3FD3Vq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Env.reset()\n",
        "Env.render(values_table=q.values_table)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPiMrYecEewu",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## 2-3. Q学習\n",
        "迷路は簡単な問題だったので、greedy方策+SARSAで十分高速に解けましたが、より難しく、探索が沢山必要な問題ではε-greedy方策を使ったほうが良い場合もあるでしょう。\n",
        "\n",
        "試してみた方はわかると思いますが、迷路サイズが大きくなると、ε-greedy方策+SARSAはやや遅いです。greedy方策+SARSAが早かったのは、\n",
        "\n",
        "$$\n",
        "\\left. \\begin{array}{l:ll:ll:ll}\n",
        "\\dots\n",
        "&\n",
        "s\n",
        "&\n",
        "{\\to}\n",
        "a\n",
        "{\\to} \n",
        "&\n",
        "s_\\text{next}\n",
        "&\n",
        "\\overset{\\pi_{greedy}(\\cdot|s_\\text{next})}{\\to}\n",
        "a_\\text{next}\n",
        "{\\to} \n",
        "&\n",
        " \\cdots\n",
        "\\\\\n",
        "&&&\n",
        "\\downarrow\n",
        "\\\\\n",
        "&\n",
        "-\n",
        "&&\n",
        "r_\\text{next}\n",
        "\\end{array} \\right.\n",
        "$$\n",
        "\n",
        "で $a_\\text{next} = \\text{argmax}_{a'}q(s_\\text{next}, a')$ が最適であり、ゴールに設定された報酬がスタートに伝搬しやすくなっているためと考えられます。\n",
        "\n",
        "そこで\n",
        "$$\n",
        "\\left. \\begin{array}{ll}\n",
        "q(s,a) &\\leftarrow \n",
        "q(s,a) - \\eta \\underbrace{ \\Big(q(s,a) - \\big[ r_\\text{next} + \\gamma q(s_\\text{next}, a_\\text{next}) \\big] \\Big)}_\\text{TD error}\n",
        "\\end{array} \\right.\n",
        "$$\n",
        "\n",
        "ではなく、\n",
        "\n",
        "$$\n",
        "\\left. \\begin{array}{ll}\n",
        "q(s,a) &\\leftarrow \n",
        "q(s,a) - \\eta  \\Big(q(s,a) - \\big[ r_\\text{next} + \\gamma \\max_{a'} q(s_\\text{next}, a') \\big] \\Big)\n",
        "\\end{array} \\right.\n",
        "$$\n",
        "\n",
        "とすることが考えられます。これを**Q学習(Q-learning)**といいます："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69avN4_yEJ_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Qlearning_optimizer(Optimizer):\n",
        "    def __init__(self, Agt, eta, gamma):\n",
        "        \"\"\"書いてください\"\"\"\n",
        "\n",
        "    def update(self, s, a, r_next, s_next):\n",
        "        \"\"\"書いてください\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULEinRpLE0ey",
        "colab_type": "text"
      },
      "source": [
        "一応デモンストレーション："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZizQFTwOExtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "q = Parameters(Env)\n",
        "Agt = Agent(Policy=EpsilonGreedy(Env=Env, Q=q))\n",
        "Opt = Qlearning_optimizer(Agt, eta=1., gamma=0.2)\n",
        "N_episode = 6000\n",
        "\n",
        "for episode in range(N_episode):\n",
        "    Env.reset()\n",
        "    while not Env.is_solved():\n",
        "        s = Env.get_state()\n",
        "        a = Agt.play()\n",
        "        s_next, r_next, _, _ = Env.step(a)\n",
        "        Opt.update(s, a, r_next, s_next)\n",
        "        if Env.t > 100: # たまにゴールできない位置に置かれてしまうので\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvroxb_ZE5jn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Env.reset()\n",
        "Env.render(values_table=q.values_table)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7ZBO8CyFf4j",
        "colab_type": "text"
      },
      "source": [
        "## ● SARSAとQ学習の違い\n",
        "SARSAは**方策オン**型、Q学習は**方策オフ**型と呼ばれます。詳細は\n",
        "[別のノートブック(迷路ではなく、崖歩きの環境)](https://github.com/AkinoriTanaka-phys/HPC-Phys_tutorial_and_hands-on/blob/master/cliff_walkers.ipynb)\n",
        "で説明していますので、興味あれば一読してみてください。\n",
        "リンク先のノートブックのセルを実行するのに必要なものはすべて実装済みです。\n",
        "実行してみたい人は以下に貼り付けたりしてみてください："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6FlN7EPFlkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qon9q2_mFD89",
        "colab_type": "text"
      },
      "source": [
        "## ● Deep Q-Network (DQN)\n",
        "**Q学習**と深層学習を組み合わせたのが**深層Q学習ネットワーク(Deep Q-Network, DQN)**です。\n",
        "今回、実装はしませんが説明だけしておきます。論文はDeepMindによる\n",
        "https://arxiv.org/abs/1312.5602\n",
        "で、アルファ碁以前の深層強化学習論文だとおもいます。\n",
        "\n",
        "これまでの\n",
        "```\n",
        "q = Parameters(Env)\n",
        "```\n",
        "では$Q(s, a)$の「テーブル」を用意して、テーブルの値をs, a毎に更新していたのでしたが、s, aの空間が巨大になると、必然的にテーブルも巨大になり、学習がはかどらなさそうです。\n",
        "\n",
        "そこで、s を入力に取り、aの個数分の出力を持った深層ニューラルネット（**Deep Q-Network**）$f_Q$を導入し\n",
        "\n",
        "$$\n",
        "f_Q(s) =\n",
        "\\begin{pmatrix}\n",
        "y_0 \n",
        "\\\\\n",
        "y_1 \n",
        "\\\\\n",
        "\\vdots\n",
        "\\\\\n",
        "y_{\\# a}\n",
        "\\end{pmatrix}\n",
        "\\approx\n",
        "\\begin{pmatrix}\n",
        "Q(s, a=0)\n",
        "\\\\\n",
        "Q(s, a=1)\n",
        "\\\\\n",
        "\\vdots\n",
        "\\\\\n",
        "Q(s, a=\\# a)\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "だと解釈します。`q = Parameters(Env)`のような (s, a) テーブルの代わりに、s$\\to$a の関数を考えるのです。\n",
        "\n",
        "ニューラルネットの出力値が$Q$の値になるようにするので、そのまま**Q学習**の考え方が使えます。Q学習は\n",
        "\n",
        "$$\n",
        "l(q) = \\frac{1}{2} \n",
        "\\Big(q(s,a) - \\big[ r_\\text{next} + \\gamma \\max_{a'}q(s_\\text{next}, a') \\big] \\Big)^2\n",
        "$$\n",
        "\n",
        "の誤差関数を減少させる学習だったので、$q(s,a) = [f_Q(s)]_a$として、ニューラルネットの誤差関数をこれに設定：\n",
        "\n",
        "$$\n",
        "l(f_Q) = \\frac{1}{2} \n",
        "\\Big([f_Q(s)]_a - \\big[ r_\\text{next} + \\gamma \\max_{a'}[f_Q(s_\\text{next})]_{a'} \\big] \\Big)^2\n",
        "$$\n",
        "\n",
        "し、勾配更新\n",
        "\n",
        "$$\n",
        "f_Q \\leftarrow f_Q - \\eta \\nabla_{f_Q} l(f_Q)\n",
        "$$\n",
        "\n",
        "すればよいのです。実際にはこれだけでなく、いくつかの工夫を更に組み合わせますが、アイデアとしては単純なものです。論文では\n",
        "* $s$:ゲームの画面のピクセル値\n",
        "* $a$:コントローラーのボタン\n",
        "\n",
        "という人間と対等な環境でAtari社のクラシックゲーム（ブロック崩しなど）を解かせています。例えば\n",
        "https://www.youtube.com/watch?v=TmPfTpjtdgg\n",
        "が公式？の動画ですが、0:50あたりまで見ると、ブロック崩しでブロックの裏側にボールを打ち出すテクニックが発見されたりして面白いです。\n",
        "また\n",
        "https://www.youtube.com/watch?v=MKtNv1UOaZA\n",
        "などで、学習中の動画が見れます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqEPabM1FPor",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# 3. 方策最適化に基づく学習アルゴリズム\n",
        "---\n",
        "価値ベースの方法は、「行動空間」`Env.action_space` が巨大すぎる場合などではトレーニングに失敗することがあります。別の選択肢の 1 つは、値 Q(s,a) ではなく、ポリシー $\\pi$ 自体のモデルを作成することです。\n",
        "\n",
        "## 3-1. 状態価値関数、期待収益、方策勾配法\n",
        "[Section2](https://github.com/AkinoriTanaka-phys/HPC-Phys_tutorial_and_hands-on/blob/master/section2.ipynb)\n",
        "では行動価値関数を\n",
        "\n",
        "$$\n",
        "Q^\\pi(s,a) = \\langle \\sum_{k=0}^\\infty \\gamma^k r_{t+1+k} \\rangle_{(s_t, a_t)=(s,a)}\n",
        "$$\n",
        "\n",
        "としました。これは状態 s で行動 a を取った時の収益の期待値だったわけですが、これを更に a について方策で期待値を取ったもの\n",
        "\n",
        "$$\n",
        "V^{\\pi}(s) = \\langle Q^{\\pi}(s, a) \\rangle_{a \\sim \\pi(\\cdot|s)}\n",
        "$$\n",
        "\n",
        "これは「方策 $\\pi$ のもとで状態 s が持っている価値」を表し、**状態価値関数(state value function)** と呼ばれます。更に、迷路の実装を思い出すと"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JNb4YkSFDHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Env = MazeEnv(5,5, 1.2, figsize=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv3ouVl-GMHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Env.reset() # 実行のたびにエージェントの位置が変わる\n",
        "Env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNQ0w0rBGTbS",
        "colab_type": "text"
      },
      "source": [
        "の**`Env.reset()`**は最初の状態 s をランダムに選んでいるのがわかります。この確率を $P_\\text{reset}(s)$ とし、状態価値の期待値を取ると\n",
        "\n",
        "$$\n",
        "J(\\pi) = \\langle V^{\\pi}(s) \\rangle_{s \\sim P_\\text{reset} (\\cdot)}\n",
        "$$\n",
        "\n",
        "となりますが、環境が固定されている以上この量は「方策にしか依存しない量」であり、かつ方策の良さを表す量（大きいほど良い）であると言えるでしょう。これを**期待収益(expected return)**といいます。\n",
        "\n",
        "### ● 方策勾配法\n",
        "そこで、価値の推定をすっ飛ばして、この$J(\\pi)$を直接最大化する学習が考えられます。そのうちでも最も単純なのが\n",
        "方策を θ でパラメーター化し\n",
        "\n",
        "$$\n",
        "\\pi(a|s) = \\pi_\\theta(a|s)\n",
        "$$\n",
        "\n",
        "この時の期待収益を $\\theta$ の関数と考え、勾配更新\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta + \\eta \\nabla_\\theta J(\\pi_{\\theta})\n",
        "$$\n",
        "\n",
        "によって学習を進めてゆくことです。これを**方策勾配法(Policy-gradient method)**といいます。\n",
        "\n",
        "問題は方策勾配 $\\nabla_\\theta J(\\pi_{\\theta})$ をどうやって求めるのか、というところです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIMeq9_LGhra",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## 3-2. 方策勾配定理とREINFORCEアルゴリズム\n",
        "ここで少し定義を変更させてください。今までは行動価値観数を\n",
        "\n",
        "$$\n",
        "Q^\\pi(s,a) = \\langle \\sum_{k=0}^\\infty \\gamma^k r_{t+1+k} \\rangle_{(s_t, a_t)=(s,a)}\n",
        "$$\n",
        "\n",
        "と減衰和にしていましたが、ここからは\n",
        "\n",
        "$$\n",
        "Q^\\pi(s,a) = \n",
        "\\lim_{T \\to \\infty}\n",
        "\\langle \\frac{1}{T}\\sum_{k=0}^{T-1}  r_{t+1+k} \\rangle_{(s_t, a_t)=(s,a)}\n",
        "$$\n",
        "\n",
        "と平均和の時間無限大極限に取ることにします。この場合のベルマン方程式は、\n",
        "\n",
        "$$\n",
        "Q^\\pi(s,a) = \\langle\n",
        "Q^\\pi(s_{+1}, a_{+1})\n",
        "\\rangle_{s_{+1} \\sim P_s(\\cdot|s, a),\\ a_{+1} \\sim \\pi(\\cdot | s_{+1})}\n",
        "$$\n",
        "\n",
        "となります。これを用いて方策勾配を求めてみましょう。まず状態価値観数の方策勾配から求めると\n",
        "\n",
        "$$\n",
        "\\left. \\begin{array}{rl}\n",
        "\\nabla_\\theta \\underbrace{ V^{\\pi_\\theta}(s) }_{\\langle Q^{\\pi_\\theta} (s, a) \\rangle_{a \\sim \\pi_\\theta(\\cdot|s)}}\n",
        "&=\n",
        "\\nabla_\\theta \n",
        "\\sum_{a}\n",
        "Q^{\\pi_\\theta} (s, a)\n",
        "\\pi_\\theta(a|s)\n",
        "\\\\\n",
        "&=\n",
        "\\sum_{a}\n",
        "\\Big(\n",
        "\\underbrace{\n",
        "\\nabla_\\theta \n",
        "Q^{\\pi_\\theta} (s, a)\n",
        "}_{(*)}\n",
        "\\cdot\n",
        "\\pi_\\theta(a|s)\n",
        "+\n",
        "Q^{\\pi_\\theta} (s, a)\n",
        "\\cdot\n",
        "\\underbrace{\n",
        "\\nabla_\\theta \n",
        "\\pi_\\theta(a| s)\n",
        "}_{\\big(\\nabla_\\theta \\log \\pi_\\theta(a | s)\\big) \\cdot \\pi_\\theta(a | s)}\n",
        "\\Big)\n",
        "\\\\\n",
        "&=\n",
        "\\sum_{a}\n",
        "\\Big(\n",
        "(*)\n",
        "+\n",
        "Q^{\\pi_\\theta} (s, a)\n",
        "\\nabla_\\theta \\log \\pi_\\theta(a | s)\n",
        "\\Big) \\cdot\n",
        "\\pi_\\theta(a|s)\n",
        "\\\\\n",
        "&=\n",
        "\\big\\langle\n",
        "(*)\n",
        "+\n",
        "Q^{\\pi_\\theta} (s, a)\n",
        "\\nabla_\\theta \\log \\pi_\\theta(a | s)\n",
        "\\big\\rangle_{a \\sim \\pi_\\theta(\\cdot|s)}\n",
        "\\end{array} \\right.\n",
        "$$\n",
        "\n",
        "となって、行動価値関数の方策勾配を計算する必要に迫られます。しかしこれはベルマン方程式のため、再帰的に計算でき\n",
        "\n",
        "$$\n",
        "\\left. \\begin{array}{rl}\n",
        "(*) &= \n",
        "\\nabla_\\theta \n",
        "\\underbrace{Q^{\\pi_\\theta} (s, a)}_{Bellman}\n",
        "\\\\\n",
        "&=\n",
        "\\nabla_\\theta \n",
        "\\langle\n",
        "Q^{\\pi_\\theta}(s_{+1}, a_{+1})\n",
        "\\rangle_{s_{+1} \\sim P_s(\\cdot|s, a),\\ a_{+1} \\sim \\pi_\\theta(\\cdot | s_{+1})}\n",
        "\\\\\n",
        "&=\n",
        "\\nabla_\\theta \n",
        "\\sum_{s_{+1}, a_{+1}}\n",
        "Q^{\\pi_\\theta}(s_{+1}, a_{+1})\n",
        "\\pi_\\theta(a_{+1} | s_{+1})\n",
        "P_s(s_{+1}|s, a)\n",
        "\\\\\n",
        "&=\n",
        "\\sum_{s_{+1}, a_{+1}}\n",
        "\\Big(\n",
        "\\underbrace{\n",
        "\\nabla_\\theta \n",
        "Q^{\\pi_\\theta}(s_{+1}, a_{+1})\n",
        "}_{(*)}\n",
        "\\cdot\n",
        "\\pi_\\theta(a_{+1} | s_{+1})\n",
        "+\n",
        "Q^{\\pi_\\theta}(s_{+1}, a_{+1})\n",
        "\\cdot\n",
        "\\underbrace{\n",
        "\\nabla_\\theta \n",
        "\\pi_\\theta(a_{+1} | s_{+1})\n",
        "}_{\\big(\\nabla_\\theta \\log \\pi_\\theta(a_{+1} | s_{+1})\\big) \\cdot \\pi_\\theta(a_{+1} | s_{+1})}\n",
        "\\Big)\n",
        "P_s(s_{+1}|s, a)\n",
        "\\\\\n",
        "&=\n",
        "\\sum_{s_{+1}, a_{+1}}\n",
        "\\Big(\n",
        "(*)\n",
        "+\n",
        "Q^{\\pi_\\theta}(s_{+1}, a_{+1})\\nabla_\\theta \\log \\pi_\\theta(a_{+1} | s_{+1})\n",
        "\\Big) \\cdot\n",
        "\\pi_\\theta(a_{+1} | s_{+1})\n",
        "P_s(s_{+1}|s, a)\n",
        "\\\\\n",
        "&=\n",
        "\\big\\langle\n",
        "(*)\n",
        "+\n",
        "Q^{\\pi_\\theta}(s_{+1}, a_{+1})\\nabla_\\theta \\log \\pi_\\theta(a_{+1} | s_{+1})\n",
        "\\big\\rangle_{a_{+1} \\sim \\pi_\\theta(\\cdot | s_{+1}),\\ s_{+1} \\sim P_s(\\cdot|s, a)}\n",
        "\\\\\n",
        "&=\n",
        "\\Big\\langle\n",
        "\\sum_{t=0}^\\infty\n",
        "Q^{\\pi_\\theta}(s_{t+1}, a_{t+1})\\nabla_\\theta \\log \\pi_\\theta(a_{t+1} | s_{t+1})\n",
        "\\Big\\rangle_{MDP,\\ (s_0, a_0)=(s, a)}\n",
        "\\end{array} \\right.\n",
        "$$\n",
        "\n",
        "と書け、(*)をもとの V の勾配に代入すると $a_t = a$ として\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta V(s)\n",
        "=\n",
        "\\Big\\langle\n",
        "\\sum_{t=0}^\\infty\n",
        "Q^{\\pi_\\theta}(s_{t}, a_{t})\\nabla_\\theta \\log \\pi_\\theta(a_{t} | s_{t})\n",
        "\\Big\\rangle_{MDP,\\ s_0=s}\n",
        "$$\n",
        "\n",
        "更に**期待収益** $J(\\pi_\\theta)$は、これの初期状態による期待値なので、結局\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\pi_\\theta)\n",
        "=\n",
        "\\Big\\langle\n",
        "\\sum_{t=0}^\\infty\n",
        "Q^{\\pi_\\theta}(s_{t}, a_{t})\\nabla_\\theta \\log \\pi_\\theta(a_{t} | s_{t})\n",
        "\\Big\\rangle_{MDP}\n",
        "$$\n",
        "\n",
        "が得られます。これを**方策勾配定理(Policy-gradient theorem)**と言います。\n",
        "\n",
        "### ● REINFORCEアルゴリズム\n",
        "実際には、期待値はサンプル$(r_t, s_t, a_t)$によって近似され\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\pi_\\theta)\n",
        "\\approx\n",
        "\\sum_{t=0}^\\infty\n",
        "Q^{\\pi_\\theta}(s_{t}, a_{t})\\nabla_\\theta \\log \\pi_\\theta(a_{t} | s_{t})\n",
        "$$\n",
        "\n",
        "とされますが、ここで結局 $Q^{\\pi_\\theta}(s_{t}, a_{t})$ が必要になってしまいました。この行動価値関数の意味は、収益の期待値だったわけですので、これを更に報酬のサンプルで適当な T (典型的にはサンプルのエピソード長さが使われる)\n",
        "\n",
        "$$\n",
        "Q^{\\pi_\\theta}(s_{t}, a_{t})\n",
        "\\approx\n",
        "\\frac{1}{T} \\sum_{k=0}^{T-1} r_{t+1+k}\n",
        "$$\n",
        "\n",
        "で近似することが考えられます。これを**REINFORCEアルゴリズム**と言います。すなわち、REINFORCEアルゴリズムとは\n",
        "1. MDPサンプル$\\{(r_t, s_t, a_t)\\}_{t=0, 2, \\dots, T-1}$を得る\n",
        "2. $\n",
        "\\theta \\leftarrow \\theta + \\eta g_\\theta,\\quad \\text{where} \\\n",
        "g_\\theta = \\sum_{t=0}^\\infty\n",
        "\\Big(\n",
        "\\frac{1}{T} \\sum_{k=0}^{T-1} r_{t+1+k}\n",
        "\\Big)\n",
        "\\nabla_\\theta \\log \\pi_\\theta(a_{t} | s_{t})\n",
        "$\n",
        "\n",
        "を繰り返して、良い方策$\\pi_\\theta$を探索する手法のことです。\n",
        "> **【補足】** MDPサンプルは一回の更新につきなるべく多く取ったほうが方策勾配$g_t$の精度が上がります。その場合はエピソード数$m = 0, 1, \\dots, M-1$として$g_\\theta = \n",
        "\\frac{1}{M} \\sum_{m=0}^{M-1}\n",
        "\\sum_{t=0}^\\infty\n",
        "\\Big(\n",
        "\\frac{1}{T} \\sum_{k=0}^{T-1} r_{t+1+k}^{(m)}\n",
        "\\Big)\n",
        "\\nabla_\\theta \\log \\pi_\\theta(a_{t}^{(m)} | s_{t}^{(m)})$です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkE9ShnDHELN",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## 3-3. Softmax方策による実装\n",
        "\n",
        "実際にREINFORCEを実装するにあたって、**Softmax方策**というのを導入します。これは適当な関数 $f(s, a)$ を用いて\n",
        "\n",
        "$$\n",
        "\\pi_f(a|s)\n",
        "=\n",
        "\\frac{\\exp\\{f(s,a)\\}}{\\sum_{a'} \\exp\\{\n",
        "f(s, a')\n",
        "\\}}\n",
        "$$\n",
        "\n",
        "とする方策です。[Section2](https://github.com/AkinoriTanaka-phys/HPC-Phys_tutorial_and_hands-on/blob/master/section2.ipynb)と同様に、$f(s,a)$をパラメータとして"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkNirO-mGPhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Env = MazeEnv(5,5, figsize=3)\n",
        "f = Parameters(Env)\n",
        "Env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAGyHF05HP8A",
        "colab_type": "text"
      },
      "source": [
        "と実装してみます。Softmax方策を実装するにあたって、まずsoftmax関数\n",
        "\n",
        "$$\n",
        "\\text{softmax}\\Big(\n",
        "\\begin{pmatrix}\n",
        "x_0 \\\\\n",
        "x_1 \\\\\n",
        "x_2 \\\\\n",
        "x_3 \\\\\n",
        "\\end{pmatrix}\n",
        "\\Big)\n",
        "=\n",
        "\\frac{1}{\n",
        "\\exp\\{x_0\\}+\\exp\\{x_1\\}+\\exp\\{x_2\\}+\\exp\\{x_3\\}\n",
        "}\n",
        "\\begin{pmatrix}\n",
        "\\exp\\{x_0\\} \\\\\n",
        "\\exp\\{x_1\\} \\\\\n",
        "\\exp\\{x_2\\} \\\\\n",
        "\\exp\\{x_3\\} \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "を実装しておきます："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5L8ohACHOoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(xs):\n",
        "    \"\"\"書いてください\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRrD10RRHX4q",
        "colab_type": "text"
      },
      "source": [
        "こうしておいて、"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR3A7-dkHW0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Softmax(Policy):\n",
        "    def __init__(self, Env, f=None, temp=1):\n",
        "        \"\"\"書いてください\"\"\"\n",
        "        \n",
        "    def get_prob_table(self):\n",
        "        \"\"\"書いてください\"\"\"\n",
        "    \n",
        "    def get_prob(self, state):\n",
        "        \"\"\"書いてください\"\"\"\n",
        "        \n",
        "    def sample(self):\n",
        "        \"\"\"書いてください\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCZdzQJfHf-D",
        "colab_type": "text"
      },
      "source": [
        "で良いでしょう。（実装では**温度**`temp`を導入して確率をコントロールできるようにしました）\n",
        "\n",
        "### ● 迷路の場合のREINFORCEアルゴリズム\n",
        "迷路の環境では、時刻$t$で\n",
        "* ゴールしていれば$r_t = 1$\n",
        "* していなければ$r_t=0$\n",
        "\n",
        "だったので、1エピソードで共通して方策勾配は\n",
        "\n",
        "$$\n",
        "g_\\theta\n",
        "=\n",
        "\\left\\{ \\begin{array}{ll}\n",
        "\\frac{1}{T} \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta (a_t|s_t) & (\\text{if solved}) \\\\\n",
        "0 & (\\text{if unsolved}) \\\\\n",
        "\\end{array} \\right.\n",
        "$$\n",
        "\n",
        "ということになります。この方策勾配はSoftmax方策だともう少し簡単化出来ます。ゴールした場合の勾配にSoftmaxを$\\theta=f(s,a)$として代入してみると\n",
        "\n",
        "$$\n",
        "\\left. \\begin{array}{ll}\n",
        "\\frac{1}{T}\n",
        "\\sum_{t=0}^{T-1} \\nabla_{f(s,a)} \\log \\pi_f (a_t|s_t) \n",
        "&=\n",
        "\\frac{1}{T}\n",
        "\\sum_{t=0}^{T-1} \\nabla_{f(s,a)} \\log \n",
        "\\frac{\\exp\\{f(s_t,a_t)\\}}{\\sum_{a'} \\exp\\{\n",
        "f(s_t, a')\n",
        "\\}}\n",
        "\\\\\n",
        "&=\n",
        "\\frac{1}{T}\n",
        "\\sum_{t=0}^{T-1} \\nabla_{f(s,a)} \n",
        "\\Big(\n",
        "f(s_t,a_t)\n",
        "-\n",
        "\\log \\sum_{a'} \\exp\\{\n",
        "f(s_t, a')\n",
        "\\}\n",
        "\\Big)\n",
        "\\\\\n",
        "&=\n",
        "\\frac{1}{T}\n",
        "\\sum_{t=0}^{T-1} \n",
        "\\Big(\n",
        "\\overbrace{ \\nabla_{f(s,a)} f(s_t, a_t)}^{\\delta_{(s,a)=(s_t, a_t)}}\n",
        "-\n",
        "\\frac{\n",
        "\\sum_{a''}\n",
        "\\overbrace{ \\nabla_{f(s,a)} f(s_t, a'')}^{\\delta_{(s,a)=(s_t, a'')}}\n",
        "\\cdot\n",
        "\\exp\\{\n",
        "f(s_t, a'')\n",
        "\\}\n",
        "}{\n",
        "\\sum_{a'} \\exp\\{\n",
        "f(s_t, a')\n",
        "\\}\n",
        "}\n",
        "\\Big)\n",
        "\\\\\n",
        "&=\n",
        "\\frac{1}{T}\n",
        "\\sum_{t=0}^{T-1} \n",
        "\\Big(\n",
        "\\delta_{(s,a)=(s_t, a_t)}\n",
        "-\n",
        "\\delta_{s=s_t}\n",
        "\\underbrace{\n",
        "\\frac{\n",
        "\\exp\\{\n",
        "f(s_t, a)\n",
        "\\}\n",
        "}{\n",
        "\\sum_{a'} \\exp\\{\n",
        "f(s_t, a')\n",
        "\\}\n",
        "}\n",
        "}_{\\pi_f(a|s_t)}\n",
        "\\Big)\n",
        "\\\\\n",
        "&=\n",
        "\\frac{1}{T}\\Big(\n",
        "N_{(s,a)}\n",
        "-\n",
        "N_s \\pi_f(a|s)\n",
        "\\Big)\n",
        "\\end{array} \\right.\n",
        "$$\n",
        "\n",
        "となるのがわかります。ここで\n",
        "* $N_{(s,a)}$:エピソード中に状態 s で行動 a を選んだ回数\n",
        "* $N_s$:エピソード中に状態 s を取った回数\n",
        "\n",
        "を表すとします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pal81FlGHdCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class REINFORCE_optimizer(Optimizer):\n",
        "    def __init__(self, Agt, eta):\n",
        "        \"\"\"書いてください\"\"\"\n",
        "            \n",
        "    def update(self, s, a, r_next, s_next):\n",
        "        \"\"\"書いてください\"\"\"\n",
        "            \n",
        "    def reset(self):\n",
        "        \"\"\"書いてください\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_R4urpuH0jU",
        "colab_type": "text"
      },
      "source": [
        "学習させてみます"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-33aNObJHsjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Env = MazeEnv(15,15, 1.2)\n",
        "Env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMUwpSnHH8Ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "f = Parameters(Env)\n",
        "Pi=Softmax(Env, f=f, temp=1)\n",
        "Agt = Agent(Pi)\n",
        "Opt = REINFORCE_optimizer(Agt, eta=10)\n",
        "N_episode = 600 # 600でも十分\n",
        "\n",
        "for episode in range(N_episode):\n",
        "    \"\"\"書いてください\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62m817xHIAVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Opt.N # 解けた回数＝実際の更新回数"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w8_FB9ZIDYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Env.reset()\n",
        "Env.play_interactive(Agt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXB7p7HGIF6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Env.reset()\n",
        "Env.render(values_table=Pi.get_prob_table())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}