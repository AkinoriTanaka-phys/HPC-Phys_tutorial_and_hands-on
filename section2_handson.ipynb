{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('bmh')\n",
    "%matplotlib inline\n",
    "\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container {width:100% !important;}</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# section1 で実装したものをインポート\n",
    "from maze import MazeEnv, a2m\n",
    "from agents import Agent\n",
    "from policies import Policy, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 価値推定に基づく学習アルゴリズム\n",
    "---\n",
    "## 2-1. 行動価値関数 $Q(s,a)$ と貪欲な方策\n",
    "方策として前節の**`Random`**でもゲームのプレイは出来ましたが、当然ムダな動きが多いので、\n",
    "* 各状態 s に対して「最適」な行動 a が存在するか？\n",
    "\n",
    "というのは自然な問いでしょう。素朴には「得られる即時報酬の総和」$\\sum_t r_t$ を最大化すれば良い気がしますが、よく取られるのは適当な減衰率 0<γ<1 を導入した、時刻t からの減衰和です：\n",
    "\n",
    "$$\n",
    "g_t = \\sum_{k=0}^\\infty \\gamma^k r_{t+1+k}\n",
    "$$\n",
    "\n",
    "これを**収益(return)**といいます。\n",
    "\n",
    "この値を MDP の系列の各時刻で最大にするようにしたいわけですが、$g_t, r_t$は確率変数なので、このままではゲームのプレイサンプル毎に値が変動してしまいます。\n",
    "\n",
    "### ● 行動価値関数 $Q(s,a)$\n",
    "そこでこれを、$P_s, P_r, \\pi$の確率から成る MDP における期待値で表すことにします。\n",
    "\n",
    "$$\n",
    "Q(s, a) = \\langle g_t \\rangle_{(s_t, a_t)=(s,a)}\n",
    "$$\n",
    "\n",
    "これを**行動価値関数(action value function)**と言います。\n",
    "MDPと並列して図示すると、\n",
    "\n",
    "$$\n",
    "\\left. \\begin{array}{l:r:ll:ll:l|l}\n",
    "\\dots\n",
    "&\n",
    "s, a\n",
    "{\\to} \n",
    "&\n",
    "s_{t+1}\n",
    "&\n",
    "\\overset{\\pi(\\cdot|s_{t+1})}{\\to}\n",
    "a_{t+1}\n",
    "{\\to} \n",
    "&\n",
    "s_{t+2}\n",
    "&\n",
    "\\overset{\\pi(\\cdot|s_{t+2})}{\\to}\n",
    "a_{t+2}\n",
    "{\\to} \n",
    "&\n",
    " \\cdots\n",
    "&\n",
    "\\\\\n",
    "&&\n",
    "\\downarrow\n",
    "&&\n",
    "\\downarrow\n",
    "\\\\\n",
    "&&\n",
    "r_{t+1}\n",
    "&&\n",
    "r_{t+2}\n",
    "\\\\ \\hline \n",
    "Q(s,a) =\n",
    "&\n",
    "\\langle\n",
    "&\n",
    "r_{t+1}\n",
    "&&\n",
    "+\\gamma r_{t+2}\n",
    "&&\n",
    "+ \\dots\n",
    "&\n",
    "\\rangle\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "のように、時刻 t で s に居て、そこで a を選んだとして、以後ずっと同じ方策$\\pi$でゲームをプレイした時に得られる**収益**の期待値を表します。\n",
    "\n",
    "> **【補足】** Q(s,a)は方策$\\pi$に依存します\n",
    "\n",
    "$Q(s,a)$の値が今回の機械学習で推定するターゲットというわけです。そのため、学習パラメータのクラスを"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameters():\n",
    "    def __init__(self, Env, init=0.01):\n",
    "        \"\"\"書いてください\"\"\"\n",
    "        \n",
    "    def get_values(self, s):\n",
    "        \"\"\"書いてください\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とでもしておきましょう。\n",
    "\n",
    "### ● greedy方策\n",
    "$Q(s,a)$が**`Parameters`**クラスとして実装できたとしたとして、$Q$の意味を思い出してみると、方策として\n",
    "\n",
    "$$\n",
    "\\pi_{greedy}(a|s)\n",
    "=\n",
    "\\delta \\Big(\n",
    "a - \n",
    "\\text{argmax}_{a'}\\big\\{ Q(s, a') \\big\\}\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "つまり、与えられた状態 s 毎に、行動価値観数が最大になる行動を取る方策を取れば良さそうです。これを**貪欲な方策(greedy policy)**と言います："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Greedy(Policy):\n",
    "    def __init__(self, Env, Q=None):\n",
    "        \"\"\"書いてください\"\"\"\n",
    "        \n",
    "    def returns_action_from(self, values):\n",
    "        \"\"\"書いてください\"\"\"\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"書いてください\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Pi = Greedy(Env=None, Q=None)\n",
    "action = Pi.returns_action_from(values=[0,100,9,10])\n",
    "action, a2m[action]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **【補足】** np.argmax(リスト)は、リストに同じ値の最大値がある場合、一番添え字の若いものを選びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ● ε-greedy方策\n",
    "greedy方策には「遊び」がないため、局所解に囚われがちです。\n",
    "そこでこれを修正した**ε-貪欲方策(ε-greedy policy)**というものがよく使われます\n",
    "\n",
    "$$\n",
    "\\pi_{\\epsilon\\text{-}greedy}(a|s)\n",
    "=\n",
    "\\left\\{ \\begin{array}{ll}\n",
    "\\pi_{greedy}(a|s) & (\\text{w/ probability}\\quad 1- \\epsilon) \\\\\n",
    "\\pi_{random}(a|s) & (\\text{w/ probability}\\quad \\epsilon)\\\\\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "> **【補足】** 強化学習における一つの大きな問題が「**探索と知識利用のトレードオフ(exploitation-exploration trade-offs)**」です。ε-greedy方策はεの値を調整することで、探索と知識利用の度合いを調整できます。\n",
    "\n",
    "Greedyとほぼ同じ実装で出来ますので、ここには書きませんが、あとで使いますので、\n",
    "\n",
    "以下を実行してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from policies import EpsilonGreedy\n",
    "Pi = EpsilonGreedy(Env=None, Q=None, epsilon=0.8)\n",
    "a = Pi.returns_action_from(values=[0,100,9,10])\n",
    "a, a2m[a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ● ここまでのまとめ\n",
    "* **`Parameters`** : $Q(s,a)$の推定値のためのクラス\n",
    "* **`Greedy(Policy)`** : 内部に`Parameters`インスタンスを持つgreedy方策\n",
    "* **`Agent`** : 内部に`Policy`インスタンスを持つエージェントのクラス\n",
    "\n",
    "を実装してきました。`Env`が迷路インスタンスだとして、\n",
    "```\n",
    "q = Parameters(Env)\n",
    "Agt = Agent(Policy=Greedy(Env=Env, Q=q))\n",
    "```\n",
    "とすれば、`Env`に対して、適当に初期化されたQの推定値qに基づくgreedyなエージェントが作れたことになります。\n",
    "\n",
    "### ● ここからやりたいこと\n",
    "この**`q=Parameters(Env)`**を$Q(s,a) = \\langle \\sum_{k=0}^\\infty \\gamma^k r_{t+1+k} \\rangle$\n",
    "に設定して迷路を解けるようにしたいわけですが、これを直接計算せず、ゲームプレイの経験から推定することを考えます。この推定を行うクラス\n",
    "* **`Optimizer`** : `Agent`を読み込み、学習させるクラス\n",
    "\n",
    "を実装し、学習は\n",
    "```\n",
    "Opt = Optimizer(Agt)\n",
    "...\n",
    "Opt.update()\n",
    "```\n",
    "のように進めるとします。`Optimizer`のプロトタイプは以下のようなものです："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, Agt):\n",
    "        self.Agt = Agt\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        なんかいい感じの処理\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下では、この具体的な実装を考えていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2-2. ベルマン方程式とSARSA\n",
    "\n",
    "### ● ベルマン方程式\n",
    "それを達成するために、$Q(s,q)$ がある漸化式を満たすことを示しましょう：\n",
    "\n",
    "$$\n",
    "\\left. \\begin{array}{ll}\n",
    "Q(s, a) \n",
    "&=\n",
    "\\langle \\underbrace{g_t}_\\text{substitute def} \\rangle_{(s_t, a_t)=(s,a)}\n",
    "\\\\&=\n",
    "\\langle \\underbrace{r_{t+1}}_{=:r_\\text{next}} + \\underbrace{\\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots}_{\\gamma g_{t+1}} \\rangle_{(s_t, a_t)=(s,a)}\n",
    "\\\\&=\n",
    "\\underbrace{ \\langle r_\\text{next} \\rangle_{(s_t, a_t)=(s,a)} }_\\text{only depends on $t$-th time evolution}\n",
    "+\n",
    "\\gamma\n",
    "\\underbrace{\\langle g_{t+1} \\rangle_{(s_t, a_t)=(s,a)}}_{\n",
    "\\big\\langle\n",
    "\\langle g_{t+1} \\rangle_{(s_{t+1}, a_{t+1})=(s_\\text{next}, a_\\text{next})}\n",
    "\\big \\rangle_{s_\\text{next} \\sim P_s(\\cdot|s, a),\\ a_\\text{next} \\sim \\pi(\\cdot | s_\\text{next})}\n",
    "}\n",
    "\\\\&=\n",
    "\\langle r_\\text{next} \\rangle_{s_\\text{next} \\sim P_s(\\cdot|s, a),\\ r_\\text{next} \\sim P_r(\\cdot|s, a, s_\\text{next})}\n",
    "+ \\gamma \\big\\langle\n",
    "\\underbrace{\\langle g_{t+1} \\rangle_{(s_{t+1}, a_{t+1})=(s_\\text{next}, a_\\text{next})}}_{Q(s_\\text{next}, a_\\text{next})}\n",
    "\\big \\rangle_{s_\\text{next} \\sim P_s(\\cdot|s, a),\\ a_\\text{next} \\sim \\pi(\\cdot | s_\\text{next})}\n",
    "\\\\&=\n",
    "\\langle r_\\text{next} \\rangle_{s_\\text{next} \\sim P_s(\\cdot|s, a),\\ r_\\text{next} \\sim P_r(\\cdot|s, a, s_\\text{next})}\n",
    "+ \\gamma \\langle\n",
    "Q(s_\\text{next}, a_\\text{next})\n",
    "\\rangle_{s_\\text{next} \\sim P_s(\\cdot|s, a),\\ a_\\text{next} \\sim \\pi(\\cdot | s_\\text{next})}\n",
    "\\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを**ベルマン方程式(Bellman equation)**と言います。\n",
    "\n",
    "> **【補足】** ここでは時間$t$は離散ですが、これを連続に拡張したほうが良い場合もあるでしょう（例えば実世界で動くエージェントの強化学習など）。連続時間への拡張は**ハミルトン-ヤコビ-ベルマン方程式(Hamilton-Jacobi-Bellman equation)**と呼ばれます。名前からわかるように古典力学におけるハミルトン-ヤコビ方程式の拡張になっているそうです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ● SARSA\n",
    "ベルマン方程式の近似として、MDPのサンプル系列\n",
    "\n",
    "$$\n",
    "\\left. \\begin{array}{l:ll:ll:ll}\n",
    "\\dots\n",
    "&\n",
    "s\n",
    "&\n",
    "{\\to}\n",
    "a\n",
    "{\\to} \n",
    "&\n",
    "s_\\text{next}\n",
    "&\n",
    "{\\to}\n",
    "a_\\text{next}\n",
    "{\\to} \n",
    "&\n",
    " \\cdots\n",
    "\\\\\n",
    "&&&\n",
    "\\downarrow\n",
    "\\\\\n",
    "&\n",
    "-\n",
    "&&\n",
    "r_\\text{next}\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "があったとき、期待値をサンプルで近似すると\n",
    "\n",
    "$$\n",
    "Q(s, a) = r_\\text{next} + \\gamma Q(s_\\text{next}, a_\\text{next})\n",
    "$$\n",
    "\n",
    "としても良さそうです。つまり$q(s,a)$をモデルとして「誤差関数」\n",
    "\n",
    "$$\n",
    "l(q) = \\frac{1}{2} \n",
    "\\Big(q(s,a) - \\big[ r_\\text{next} + \\gamma q(s_\\text{next}, a_\\text{next}) \\big] \\Big)^2\n",
    "$$\n",
    "\n",
    "を減らせば良さそうです。ニューラルネットの学習よろしく、勾配法でアップデートすることにすれば\n",
    "\n",
    "$$\n",
    "\\left. \\begin{array}{ll}\n",
    "q(s,a) &\\leftarrow \n",
    "q(s,a) - \\eta \\nabla_{q(s,a)} l(q)\n",
    "\\\\\n",
    "&= \n",
    "q(s,a) - \\eta \\underbrace{ \\Big(q(s,a) - \\big[ r_\\text{next} + \\gamma q(s_\\text{next}, a_\\text{next}) \\big] \\Big)}_\\text{TD error}\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "を実際のMDPでのサンプル毎に更新すればよいでしょう。\n",
    "更新する分の部分を**Temporal Difference error(TD error)**と言います。\n",
    "TD errorの部分に順に$s, a, r_\\text{next}, s_\\text{next}, a_\\text{next}$が出てきていることから、この方法を**SARSA**といいます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SARSA_optimizer(Optimizer):\n",
    "    def __init__(self, Agt, eta, gamma):\n",
    "        \"\"\"書いてください\"\"\"\n",
    "\n",
    "    def update(self, s, a, r_next, s_next):\n",
    "        \"\"\"書いてください\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "早速これで強化学習させてみます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "Env = MazeEnv(5,5, threshold=1.2)\n",
    "Env.reset()\n",
    "Env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "q = Parameters(Env)\n",
    "Agt = Agent(Policy=Greedy(Env=Env, Q=q))\n",
    "Opt = SARSA_optimizer(Agt, eta=1., gamma=0.2)\n",
    "N_episode = 6000\n",
    "\n",
    "\"\"\" ##### アニメーションする場合 最初に #\n",
    "%matplotlib notebook \n",
    "from maze import get_fig_ax\n",
    "fig, ax = get_fig_ax()\n",
    "N_episode = 30\n",
    "#\"\"\"\n",
    "\n",
    "for episode in range(N_episode):\n",
    "    \"\"\"書いてください\"\"\"\n",
    "    \n",
    "\"\"\" ##### アニメーションする場合 一個目の#抜く\n",
    "        Env.render(fig=fig, ax=ax, values_table=q.values_table)\n",
    "        \n",
    "%matplotlib inline\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習後のエージェントで迷路を解かせてみます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Env.reset()\n",
    "%matplotlib notebook \n",
    "Env.play_interactive(Agt)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Env.render()`に推定された行動価値関数を図示するオプション\n",
    "* `Env.render(values_table)`\n",
    "\n",
    "を用意してみました。\n",
    "濃い色ほど$Q(s,a)$の値が大きいことを意味します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Env.reset()\n",
    "Env.render(values_table=q.values_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2-3. Q学習\n",
    "迷路は簡単な問題だったので、greedy方策+SARSAで十分高速に解けましたが、より難しく、探索が沢山必要な問題ではε-greedy方策を使ったほうが良い場合もあるでしょう。\n",
    "\n",
    "試してみた方はわかると思いますが、迷路サイズが大きくなると、ε-greedy方策+SARSAはやや遅いです。greedy方策+SARSAが早かったのは、\n",
    "\n",
    "$$\n",
    "\\left. \\begin{array}{l:ll:ll:ll}\n",
    "\\dots\n",
    "&\n",
    "s\n",
    "&\n",
    "{\\to}\n",
    "a\n",
    "{\\to} \n",
    "&\n",
    "s_\\text{next}\n",
    "&\n",
    "\\overset{\\pi_{greedy}(\\cdot|s_\\text{next})}{\\to}\n",
    "a_\\text{next}\n",
    "{\\to} \n",
    "&\n",
    " \\cdots\n",
    "\\\\\n",
    "&&&\n",
    "\\downarrow\n",
    "\\\\\n",
    "&\n",
    "-\n",
    "&&\n",
    "r_\\text{next}\n",
    "\\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "で $a_\\text{next} = \\text{argmax}_{a'}q(s_\\text{next}, a')$ が最適であり、ゴールに設定された報酬がスタートに伝搬しやすくなっているためと考えられます。\n",
    "\n",
    "そこで\n",
    "$$\n",
    "\\left. \\begin{array}{ll}\n",
    "q(s,a) &\\leftarrow \n",
    "q(s,a) - \\eta \\underbrace{ \\Big(q(s,a) - \\big[ r_\\text{next} + \\gamma q(s_\\text{next}, a_\\text{next}) \\big] \\Big)}_\\text{TD error}\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "ではなく、\n",
    "\n",
    "$$\n",
    "\\left. \\begin{array}{ll}\n",
    "q(s,a) &\\leftarrow \n",
    "q(s,a) - \\eta  \\Big(q(s,a) - \\big[ r_\\text{next} + \\gamma \\max_{a'} q(s_\\text{next}, a') \\big] \\Big)\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "とすることが考えられます。これを**Q学習(Q-learning)**といいます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qlearning_optimizer(Optimizer):\n",
    "    def __init__(self, Agt, eta, gamma):\n",
    "        \"\"\"書いてください\"\"\"\n",
    "\n",
    "    def update(self, s, a, r_next, s_next):\n",
    "        \"\"\"書いてください\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一応デモンストレーション："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "q = Parameters(Env)\n",
    "Agt = Agent(Policy=EpsilonGreedy(Env=Env, Q=q))\n",
    "Opt = Qlearning_optimizer(Agt, eta=1., gamma=0.2)\n",
    "N_episode = 6000\n",
    "\n",
    "\"\"\" ##### アニメーションする場合 最初に #\n",
    "%matplotlib notebook \n",
    "from maze import get_fig_ax\n",
    "fig, ax = get_fig_ax()\n",
    "N_episode = 30\n",
    "#\"\"\"\n",
    "\n",
    "for episode in range(N_episode):\n",
    "    Env.reset()\n",
    "    while not Env.is_solved():\n",
    "        s = Env.get_state()\n",
    "        a = Agt.play()\n",
    "        s_next, r_next, _, _ = Env.step(a)\n",
    "        Opt.update(s, a, r_next, s_next)\n",
    "        if Env.t > 100: # たまにゴールできない位置に置かれてしまうので\n",
    "            break\n",
    "    \n",
    "\"\"\" ##### アニメーションする場合 一個目の#抜く\n",
    "        Env.render(fig=fig, ax=ax, values_table=q.values_table)\n",
    "        \n",
    "%matplotlib inline\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Env.reset()\n",
    "Env.render(values_table=q.values_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ● SARSAとQ学習の違い\n",
    "SARSAは**方策オン**型、Q学習は**方策オフ**型と呼ばれます。詳細は\n",
    "[別のノートブック(迷路ではなく、崖歩きの環境)](https://github.com/AkinoriTanaka-phys/HPC-Phys_tutorial_and_hands-on/blob/master/cliff_walkers.ipynb)\n",
    "で説明していますので、興味あれば一読してみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ● Deep Q-Network (DQN)\n",
    "**Q学習**と深層学習を組み合わせたのが**深層Q学習ネットワーク(Deep Q-Network, DQN)**です。\n",
    "今回、実装はしませんが説明だけしておきます。論文はDeepMindによる\n",
    "https://arxiv.org/abs/1312.5602\n",
    "で、アルファ碁以前の深層強化学習論文だとおもいます。\n",
    "\n",
    "これまでの\n",
    "```\n",
    "q = Parameters(Env)\n",
    "```\n",
    "では$Q(s, a)$の「テーブル」を用意して、テーブルの値をs, a毎に更新していたのでしたが、s, aの空間が巨大になると、必然的にテーブルも巨大になり、学習がはかどらなさそうです。\n",
    "\n",
    "そこで、s を入力に取り、aの個数分の出力を持った深層ニューラルネット（**Deep Q-Network**）$f_Q$を導入し\n",
    "\n",
    "$$\n",
    "f_Q(s) =\n",
    "\\begin{pmatrix}\n",
    "y_0 \n",
    "\\\\\n",
    "y_1 \n",
    "\\\\\n",
    "\\vdots\n",
    "\\\\\n",
    "y_{\\# a}\n",
    "\\end{pmatrix}\n",
    "\\approx\n",
    "\\begin{pmatrix}\n",
    "Q(s, a=0)\n",
    "\\\\\n",
    "Q(s, a=1)\n",
    "\\\\\n",
    "\\vdots\n",
    "\\\\\n",
    "Q(s, a=\\# a)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "だと解釈します。`q = Parameters(Env)`のような (s, a) テーブルの代わりに、s$\\to$a の関数を考えるのです。\n",
    "\n",
    "ニューラルネットの出力値が$Q$の値になるようにするので、そのまま**Q学習**の考え方が使えます。Q学習は\n",
    "\n",
    "$$\n",
    "l(q) = \\frac{1}{2} \n",
    "\\Big(q(s,a) - \\big[ r_\\text{next} + \\gamma \\max_{a'}q(s_\\text{next}, a') \\big] \\Big)^2\n",
    "$$\n",
    "\n",
    "の誤差関数を減少させる学習だったので、$q(s,a) = [f_Q(s)]_a$として、ニューラルネットの誤差関数をこれに設定：\n",
    "\n",
    "$$\n",
    "l(f_Q) = \\frac{1}{2} \n",
    "\\Big([f_Q(s)]_a - \\big[ r_\\text{next} + \\gamma \\max_{a'}[f_Q(s_\\text{next})]_{a'} \\big] \\Big)^2\n",
    "$$\n",
    "\n",
    "し、勾配更新\n",
    "\n",
    "$$\n",
    "f_Q \\leftarrow f_Q - \\eta \\nabla_{f_Q} l(f_Q)\n",
    "$$\n",
    "\n",
    "すればよいのです。実際にはこれだけでなく、いくつかの工夫を更に組み合わせますが、アイデアとしては単純なものです。論文では\n",
    "* $s$:ゲームの画面のピクセル値\n",
    "* $a$:コントローラーのボタン\n",
    "\n",
    "という人間と対等な環境でAtari社のクラシックゲーム（ブロック崩しなど）を解かせています。例えば\n",
    "https://www.youtube.com/watch?v=TmPfTpjtdgg\n",
    "が公式？の動画ですが、0:50あたりまで見ると、ブロック崩しでブロックの裏側にボールを打ち出すテクニックが発見されたりして面白いです。\n",
    "また\n",
    "https://www.youtube.com/watch?v=MKtNv1UOaZA\n",
    "などで、学習中の動画が見れます。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
